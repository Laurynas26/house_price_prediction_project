# ðŸ  House Price Prediction Project

An end-to-end machine learning system for predicting residential property prices in Amsterdam, built to mirror a realistic applied ML workflowâ€”from data collection and experimentation to production-ready inference.

The project evolves from exploratory notebooks into a *modular, reproducible, production-oriented codebase*, with a strong emphasis on *transparent model selection, trainâ€“inference parity, and decision traceability*.

It covers *data collection, preprocessing, feature engineering, model training, hyperparameter tuning, production model selection, inference API, and a Vite+React frontend*.

The project was motivated by a practical question: _how accurately can market listing prices be explained by structured property features, and where do listings appear over- or under-valued?_


---

## â­ Key Features / Highlights

- Config-driven preprocessing and feature engineering
- Leakage-safe, fold-wise cross-validation
- Support for baseline and extended/engineered features
- Hyperparameter optimization with Optuna for XGBoost, Random Forest, and Linear Regression
- Unified evaluation interface (`ModelEvaluator`) for train/val/test metrics
- MLflow integration for experiment tracking, metrics, hyperparameters, and model artifacts
- Explicit production model selection with audit trail (`approved_model.yaml`)
- Serverless inference with AWS Lambda and lazy pipeline initialization
- Vite + React frontend for prediction consumption

---

## ðŸ“Œ Project Overview

The repository implements a complete applied ML workflow:

1. Web scraping of [Funda.nl](https://www.funda.nl) listings
2. Config-driven preprocessing & feature engineering
3. Model benchmarking & hyperparameter tuning ([Optuna](https://optuna.org))
4. Experiment tracking & model versioning ([MLflow](https://mlflow.org))
5. Explicit production model selection
6. Model loading for inference
7. Deployment interfaces (API / AWS Lambda / Frontend)

Exploratory analysis is performed in notebooks, later *refactored into scripts and reusable modules*.

---

## âš ï¸ Data Disclaimer

- All data is collected from Funda.nl for **educational and research purposes only**.
- **Do not redistribute raw scraped data**.
- Tools and explanation of how to collect data are shared, but the data itself is not shared.
- This project **does not endorse commercial scraping** or usage outside personal learning.

---

## ðŸ—‚ï¸ Repository Structure

```text
house_price_prediction_project/
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ preprocessing_config.yaml
â”‚ â””â”€â”€ model_config.yaml
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ parsed_json/ # Parsed listing-level data
â”‚
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ mlruns/ # MLflow tracking
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ [01_modelling_main.ipynb](notebooks/01_modelling_main.ipynb)
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ [run_optuna.py](scripts/run_optuna.py)
â”‚ â”œâ”€â”€ [train_best_model.py](scripts/train_best_model.py)
â”‚ â”œâ”€â”€ [select_and_train_best_model.py](scripts/select_and_train_best_model.py)
â”‚ â””â”€â”€ [load_production_model.py](scripts/load_production_model.py)
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ scraper/ # Data collection (Web scraping)
â”‚ â”œâ”€â”€ data_loading/ # Parsing & preprocessing
â”‚ â”œâ”€â”€ features/ # Feature engineering
â”‚ â”œâ”€â”€ model/ # Training, evaluation, MLflow
â”‚ â”œâ”€â”€ utils/ # Shared utilities
â”‚ â”œâ”€â”€ api/ # Inference API
â”‚ â””â”€â”€ aws_lambda/ # Lambda deployment
â”‚
â”œâ”€â”€ frontend/ # Vite + React frontend
â”œâ”€â”€ scrape_funda_url_for_data.py
â””â”€â”€ README.md
```
---

## ðŸ” Data Collection (`src/scraper`)

- Modular, reusable web scraping components
- Separation between URL discovery, HTML parsing, and data extraction
- Rate-limit and failure tolerant
- Outputs structured JSON

> `scrape_funda_url_for_data.py` is a utility script for generating listing URLs.
> Not part of production inference, but useful for retraining or exploration.
> There is a README.md file for this folder going deeper.

---

## ðŸ§¼ Data Loading & Preprocessing (`src/data_loading`)

Responsibilities:

- Loading raw JSON listings
- Schema normalization & type coercion
- Missing value handling
- Preprocessing for modeling
- Cache management (`CacheManager`)

Principles:

- No model assumptions
- Fully config-driven
- Reusable across experiments & inference

> Note: On AWS Lambda, the preprocessing pipeline uses lazy initialization and can load a cached geolocation and amenities dataset for fast inference, avoiding large cold-start overheads.

---

## ðŸ§  Feature Engineering (`src/features`)

- Centralized feature preparation logic
- Supports baseline and extended/engineered features
- CV-aware train/validation/test splits
- Behavior controlled via `model_config.yaml`

Benefits:

- Fair feature comparisons
- Reproducible experiments
- Minimal code duplication

> `inference_meta.pkl` ensures production predictions align with training-time feature schema.
> There is a README.md file for this folder going deeper.

---

## Modeling & Evaluation (`src/model`)

### ModelEvaluator
- Unified interface supporting sklearn & XGBoost
- Train / validation / test metrics
- Optional target transformation (e.g., log-scale)
- Early-stopping support for XGBoost

### Hyperparameter Optimization
- Optuna objectives with leakage-safe K-Fold CV
- Supports Random Forest, Linear Regression, and XGBoost
- Fold-wise feature engineering & optional geo/amenities enrichment

### MLflow Logging
- Centralized logging via `MLFlowLogger`
- Metrics, hyperparameters, model artifacts
- Local tracking in `logs/mlruns`

> Note: ModelEvaluator handles metrics for train/validation/test splits, target transformations, and early stopping. MLflow logging ensures reproducibility.
> ModelEvaluator ensures all training and evaluation metrics are consistent, while MLflow provides a transparent experiment history.

---

## ðŸ“Š Experimentation & Model Selection

### Notebooks
- [`notebooks/01_modelling_main.ipynb`](notebooks/01_modelling_main.ipynb)
  - Model benchmarking
  - Feature addition experiments
  - Hyperparameter tuning
  - Overfitting analysis
  - Final model comparison

Other notebooks are exploratory or testing.
- [`notebooks/06_trying_new_feature_expansion.ipynb`](notebooks/06_trying_new_feature_expansion.ipynb) generates `df_with_lat_lon_encoded.csv`, `amsterdam_amenities.csv`, etc. If new data is scraped, these must be regenerated.

### Scripts

| Script | Purpose |
|--------|---------|
| [`run_optuna.py`](scripts/run_optuna.py) | Hyperparameter tuning |
| [`train_best_model.py`](scripts/train_best_model.py) | Train fixed-config models |
| [`select_and_train_best_model.py`](scripts/select_and_train_best_model.py) | Select & retrain production model |
| [`load_production_model.py`](scripts/load_production_model.py) | Load production model |

---

## ðŸ§ª Model Selection Strategy

- Production model is *not chosen solely by test metric*
- Selection criteria:
  - Test performance (RMSE, MAE, MAPE)
  - Trainâ€“test gap
  - Stability across folds
  - Overfitting behavior

- Explicitly defined in:
  - `config/model_config.yaml`
  - `src/model/approved_model.yaml`
  - `notebooks/01_modelling_main.ipynb`

Ensures deterministic loading and clear audit trail.

---

## ðŸš€ Inference & Deployment

### `src/api`
- Lightweight inference layer
- Designed for backend or frontend consumption
- Decoupled from training logic
> There is a README.md file for this folder going deeper (src/api/core).

### `src/aws_lambda`
- AWS Lambda-compatible wrapper
- Lazy pipeline initialization for fast cold-start
- Serverless deployment
> There is a README.md file for this folder going deeper.

### `frontend/`
- Vite + React application consuming prediction API
- Separate from ML training code

> Note: To run the Vite+React frontend locally, run the following three commands in the terminal:

- cd frontend
- npm install
- npm run dev

> I did not push to github node modules or build output. Nor the secrets/api keys.
---

## ðŸ–¼ Pipeline Diagram

```text
Funda.nl URLs
â”‚
â–¼
Scraper (src/scraper)
â”‚
â–¼
Data Loading & Preprocessing (src/data_loading)
â”‚
â–¼
Feature Engineering (src/features)
â”‚
â–¼
Model Training & Tuning (src/model + Optuna)
â”‚
â–¼
MLflow Logging & Selection
â”‚
â–¼
Production Model (model_config.yaml/approved_model.yaml)
â”‚
â–¼
Inference API / AWS Lambda (src/api + aws_lambda)
â”‚
â–¼
Frontend (Vite + React)
```

---

## â–¶ï¸ Running Locally

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Prepare data

Place Funda URLs in config/house_pages_scraped.txt

Example URL:
https://www.funda.nl/detail/koop/amsterdam/appartement-bilderdijkkade-75-b/43179082/

### 3. Choose workflow

#### a) Exploration (recommended)

- Run notebooks/01_modelling_main.ipynb
- Discover outliers & feature trade-offs
- Identify best production model

#### b) Automated

- Run scripts sequentially:
- 01_train_baselines.py
- 02_run_optuna.py
- 03_select_and_train_best_model.py
- 04_load_production_model.py

Update production model references in:

config/model_config.yaml

src/model/approved_model.yaml

Pipeline: Scraper â†’ Feature Pipeline â†’ Training Scripts â†’ MLflow â†’ Production Model â†’ Lambda API

### 4. Design Principles

- Config over hardcoding
- Explicit pipelines over magic
- Notebooks for discovery, scripts for execution
- Reproducibility first
- Production realism without over-engineering

### 5. ðŸ§° CI/CD & Dev Tools

- .github/workflows/python-tests.yml contains CI/CD pipelines for automated testing, linting and formatting
- Dockerfile for containerized execution (used in AWS Lambda)
- .pre-commit-config.yaml for code quality enforcement
- requirements-dev.txt for development dependencies
- buildspec.yaml for AWS build pipelines

Included to improve reproducibility, testing, and deployment, though the system can also run locally without them.

### 6. ðŸ§ª Testing

The project includes targeted unit tests that focus on inference safety, preprocessing correctness, and API-level behavior.

Tests cover:

- Preprocessing functions and edge cases, including neighborhood details parsing.

- API-level model prediction logic (mocked), ensuring:

1. Feature schema enforcement
2. Proper handling of missing or extra features
3. Failures when the model is misconfigured or uninitialized

> Tests validate contracts and invariants, not model performance.

#### ðŸ§ª Testing Philosophy

This project emphasizes contract-based testing rather than accuracy metrics. The goal is to ensure that:

- Preprocessing behaves deterministically.
- Feature schemas remain stable between training and inference.
- API-level inference behaves safely.
- Failures in assumptions (e.g., missing features, uninitialized model) are caught explicitly.

> This approach ensures correctness of inference, which is more critical than metrics for safe deployment.

#### What is intentionally not tested

- Model accuracy, performance metrics, or business outcomes.
- Full end-to-end integration with real file system or MLflow artifacts.
- MLflow internals, cloud infrastructure, or deployment behavior.

These aspects are validated through:

- Exploratory notebooks.
- MLflow experiment tracking.
- Manual inspection during model selection.

### ðŸ“¬ Notes

- System scales without rewriting core logic
- Prioritizes clarity, traceability, and correctness

### Contact

For questions, collaboration, or discussion, feel free to reach out.
