{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "tracking_uri = \"../logs/mlruns\"\n",
    "os.makedirs(os.path.join(tracking_uri, \".trash\"), exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"house_price_prediction\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Adjust the path to your project root folder\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(\"..\")\n",
    ")  # from notebooks/ up one level\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.data_loading.data_loading.data_loader import load_data_from_json\n",
    "from src.data_loading.preprocessing.preprocessing import preprocess_df\n",
    "from src.data_loading.preprocessing.imputation import impute_missing_values\n",
    "\n",
    "\n",
    "# go two levels up from notebook dir -> project root\n",
    "ROOT = (\n",
    "    Path(__file__).resolve().parents[2]\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parents[1]\n",
    ")\n",
    "CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"preprocessing_config.yaml\"\n",
    ")\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "df_raw = load_data_from_json(\"../data/parsed_json/*.json\")\n",
    "df_clean = preprocess_df(\n",
    "    df_raw,\n",
    "    drop_raw=CONFIG[\"preprocessing\"][\"drop_raw\"],\n",
    "    numeric_cols=CONFIG[\"preprocessing\"][\"numeric_cols\"],\n",
    ")\n",
    "df_clean = impute_missing_values(\n",
    "    df_clean, CONFIG[\"preprocessing\"][\"imputation\"]\n",
    ")\n",
    "# Drop price_num NaNs for the training of the model\n",
    "df_clean = df_clean[df_clean[\"price_num\"].notna()]\n",
    "df_clean.drop(columns=[\"living_area\"], inplace=True)\n",
    "\n",
    "# df_clean = df_clean[:100] \n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b54ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.data_prep_for_modelling.data_preparation import prepare_data\n",
    "\n",
    "FEATURES_CONFIG_PATH = (\n",
    "    ROOT / \"house_price_prediction_project\" / \"config\" / \"model_config.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.evaluate import ModelEvaluator\n",
    "from src.model.mlflow_logger import MLFlowLogger\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "logger = MLFlowLogger()\n",
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "# --- Prepare data for final modeling ---\n",
    "X_train, X_test, y_train, y_test, X_val, y_val, scaler, feature_encoders = prepare_data(\n",
    "    df=df_clean,\n",
    "    config_path=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_name=\"xgboost_early_stopping\",  \n",
    "    use_extended_features=True,           \n",
    "    cv=False                              \n",
    ")\n",
    "\n",
    "X_train_final = X_train.copy()\n",
    "X_test_final = X_test.copy()\n",
    "X_val_final = X_val.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train_final.shape)\n",
    "print(\"validation shape:\", X_val_final.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.model.objectives_optuna import unified_objective\n",
    "\n",
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "study_xgb = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "objective_xgb_partial = partial(\n",
    "    unified_objective,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng\",\n",
    "    df=df_clean,\n",
    "    features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    use_log=True,  \n",
    "    n_splits=5,\n",
    "    use_extended_features=True\n",
    ")\n",
    "study_xgb.optimize(objective_xgb_partial, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d79ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator with log-transform if used\n",
    "evaluator = ModelEvaluator(target_transform=np.log1p, inverse_transform=np.expm1)\n",
    "\n",
    "# # --- Random Forest ---\n",
    "# best_rf = RandomForestRegressor(**study_rf.best_params)\n",
    "# trained_rf, y_train_pred, y_val_pred, y_test_pred, results_rf = evaluator.evaluate(\n",
    "#     model=best_rf,\n",
    "#     X_train=X_train_final,\n",
    "#     y_train=y_train,\n",
    "#     X_test=X_test_final,\n",
    "#     y_test=y_test,\n",
    "#     X_val=X_val_final,\n",
    "#     y_val=y_val,\n",
    "#     use_xgb_train=False,\n",
    "# )\n",
    "# logger.log_model(trained_rf, \"RF_LogTransform_Optuna_feature_eng\", results_rf, use_xgb_train=False)\n",
    "\n",
    "# --- XGBoost ---\n",
    "best_xgb_params = study_xgb.best_params\n",
    "trained_xgb, y_train_pred, y_val_pred, y_test_pred, results_xgb = evaluator.evaluate(\n",
    "    model=None,  # not used in XGBoost.train\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_final,\n",
    "    y_test=y_test,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    use_xgb_train=True,\n",
    "    model_params=best_xgb_params,  # <--- crucial\n",
    "    fit_params={\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    ")\n",
    "logger.log_model(trained_xgb, \"XGB_Optuna_LogTransformed_feature_eng\", results_xgb, use_xgb_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b9bbd",
   "metadata": {},
   "source": [
    "## Extra features: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a7409",
   "metadata": {},
   "source": [
    "#### Distance to Center (Dam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095541aa",
   "metadata": {},
   "source": [
    "Getting lat and long for my listings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "# import time\n",
    "\n",
    "# df = df_clean.copy()\n",
    "# addresses = df[\"address\"].unique()\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"house_price_project\")\n",
    "# lat_lon_cache = {}\n",
    "\n",
    "# for addr in addresses:\n",
    "#     try:\n",
    "#         location = geolocator.geocode(addr)\n",
    "#         if location:\n",
    "#             lat_lon_cache[addr] = (location.latitude, location.longitude)\n",
    "#         else:\n",
    "#             lat_lon_cache[addr] = (None, None)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error for {addr}: {e}\")\n",
    "#         lat_lon_cache[addr] = (None, None)\n",
    "#     time.sleep(1)  # Respect Nominatim rate limit\n",
    "\n",
    "# Map back to original dataframe\n",
    "# df[\"lat\"] = df[\"address\"].map(lambda x: lat_lon_cache[x][0])\n",
    "# df[\"lon\"] = df[\"address\"].map(lambda x: lat_lon_cache[x][1])\n",
    "\n",
    "# pd.DataFrame(lat_lon_cache.items(), columns=[\"address\", \"lat_lon\"]).to_csv(\"geocode_cache.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40684b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_addresses = [addr for addr, (lat, lon) in lat_lon_cache.items() if lat is None or lon is None]\n",
    "\n",
    "# for addr in failed_addresses:\n",
    "#     try:\n",
    "#         location = geolocator.geocode(addr)\n",
    "#         if location:\n",
    "#             lat_lon_cache[addr] = (location.latitude, location.longitude)\n",
    "#         else:\n",
    "#             lat_lon_cache[addr] = (None, None)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Retry error for {addr}: {e}\")\n",
    "#         lat_lon_cache[addr] = (None, None)\n",
    "#     time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ee10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of addresses still missing lat/lon\n",
    "# failed_addresses = [addr for addr, (lat, lon) in lat_lon_cache.items() if lat is None or lon is None]\n",
    "\n",
    "# # Subset the dataframe for these addresses\n",
    "# df_failed = df[df['address'].isin(failed_addresses)]\n",
    "\n",
    "# # Check their postal codes\n",
    "# df_failed[['address', 'postal_code_clean']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ccb06",
   "metadata": {},
   "source": [
    "Distance to Ams city center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- City center coordinates (Dam Square, Amsterdam) ---\n",
    "city_center = (52.3730, 4.8923)\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Great-circle distance in meters between two points.\"\"\"\n",
    "    R = 6371000  # meters\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2)**2\n",
    "    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "\n",
    "# --- Step 1. Postal code centroids ---\n",
    "postal_code_coords = (\n",
    "    df[df['lat'].notna() & df['lon'].notna()]\n",
    "    .groupby('postal_code_clean')[['lat','lon']]\n",
    "    .mean()\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "# --- Step 2. Neighborhood centroids (if available) ---\n",
    "if 'neighborhood' in df.columns:\n",
    "    neighborhood_coords = (\n",
    "        df[df['lat'].notna() & df['lon'].notna()]\n",
    "        .groupby('neighborhood')[['lat','lon']]\n",
    "        .mean()\n",
    "        .to_dict(orient='index')\n",
    "    )\n",
    "else:\n",
    "    neighborhood_coords = {}\n",
    "\n",
    "# --- Step 3. Fill missing with postal → neighborhood → fallback ---\n",
    "for addr in df[df['lat'].isna()]['address']:\n",
    "    postal = df.loc[df['address'] == addr, 'postal_code_clean'].values[0]\n",
    "    neigh = df.loc[df['address'] == addr, 'neighborhood'].values[0] if 'neighborhood' in df else None\n",
    "\n",
    "    if postal in postal_code_coords:\n",
    "        lat_lon_cache[addr] = (postal_code_coords[postal]['lat'], postal_code_coords[postal]['lon'])\n",
    "    elif neigh and neigh in neighborhood_coords:\n",
    "        lat_lon_cache[addr] = (neighborhood_coords[neigh]['lat'], neighborhood_coords[neigh]['lon'])\n",
    "    else:\n",
    "        lat_lon_cache[addr] = (None, None)\n",
    "\n",
    "# --- Step 4. Map back to DataFrame ---\n",
    "df['lat'] = df['address'].map(lambda x: lat_lon_cache[x][0])\n",
    "df['lon'] = df['address'].map(lambda x: lat_lon_cache[x][1])\n",
    "\n",
    "# --- Step 5. Final fallback: mark missing and fill with -1 ---\n",
    "df['lat_lon_missing'] = df['lat'].isna().astype(int)\n",
    "df['lat'] = df['lat'].fillna(-1)\n",
    "df['lon'] = df['lon'].fillna(-1)\n",
    "\n",
    "# --- Step 6. Compute distance to city center ---\n",
    "df['dist_to_center_m'] = df.apply(\n",
    "    lambda row: haversine(row['lat'], row['lon'], city_center[0], city_center[1])\n",
    "    if row['lat'] != -1 and row['lon'] != -1 else -1,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Step 7. Create distance bins (categorical feature) ---\n",
    "bins = [-1, 0, 2000, 5000, 10000, 20000, np.inf]  # -1 kept separate\n",
    "labels = [\"missing\", \"0–2km\", \"2–5km\", \"5–10km\", \"10–20km\", \"20km+\"]\n",
    "df['dist_to_center_bin'] = pd.cut(df['dist_to_center_m'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# --- Step 8: one-hot encode the bins ---\n",
    "dist_bin_dummies = pd.get_dummies(\n",
    "    df[\"dist_to_center_bin\"], prefix=\"distbin\"\n",
    ")\n",
    "\n",
    "# --- Step 9: merge dummies back into df ---\n",
    "df = pd.concat([df, dist_bin_dummies], axis=1)\n",
    "\n",
    "# --- Step 10 (optional): drop the raw categorical bin if you don’t need it ---\n",
    "# df = df.drop(columns=[\"dist_to_center_bin\"])\n",
    "\n",
    "df.loc[df[\"dist_to_center_m\"].isna(), \"dist_to_center_bin\"] = \"missing\"\n",
    "df = df.drop(columns=[\"dist_to_center_bin\"])\n",
    "print(df[[\"dist_to_center_m\"] + list(dist_bin_dummies.columns)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/df_with_lat_lon_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/df_with_lat_lon_encoded.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1905b",
   "metadata": {},
   "source": [
    "#### Distance to amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75670467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import overpy\n",
    "import pandas as pd\n",
    "\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# Amsterdam bounding box\n",
    "bbox = (52.3100, 4.7680, 52.4100, 4.9500)  # Approximate Amsterdam area\n",
    "\n",
    "# Amenity queries: lists allow multiple key=value pairs per amenity\n",
    "amenity_queries = {\n",
    "    \"school\": [\"amenity=school\"],\n",
    "    \"park\": [\"leisure=park\"],\n",
    "    \"hospital\": [\"amenity=hospital\"],\n",
    "    \"supermarket\": [\"shop=supermarket\"],\n",
    "    \"bus_stop\": [\"highway=bus_stop\"],\n",
    "    \"tram_stop\": [\"railway=tram_stop\"],\n",
    "    \"metro_stop\": [\"railway=subway_entrance\", \"station=subway\"],\n",
    "    \"light_rail_stop\": [\"railway=light_rail\"]\n",
    "}\n",
    "\n",
    "all_amenities = []\n",
    "\n",
    "for amenity_name, osm_tags in amenity_queries.items():\n",
    "    for osm_tag in osm_tags:\n",
    "        key, value = osm_tag.split(\"=\")\n",
    "        query = f\"\"\"\n",
    "        node[{key}={value}]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "        out;\n",
    "        \"\"\"\n",
    "        print(f\"Fetching {amenity_name} ({osm_tag})...\")\n",
    "        try:\n",
    "            result = api.query(query)\n",
    "            for node in result.nodes:\n",
    "                all_amenities.append({\n",
    "                    \"amenity_type\": amenity_name,\n",
    "                    \"lat\": float(node.lat),\n",
    "                    \"lon\": float(node.lon)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {amenity_name} ({osm_tag}): {e}\")\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "amenities_df = pd.DataFrame(all_amenities)\n",
    "print(amenities_df.head())\n",
    "amenities_df.to_csv(\"../data/amsterdam_amenities.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16456ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# --- 1. Convert degrees to radians ---\n",
    "def deg2rad(df, lat_col='lat', lon_col='lon'):\n",
    "    return np.radians(df[[lat_col, lon_col]].values)\n",
    "\n",
    "# --- 2. Prepare listing coordinates ---\n",
    "listing_coords_rad = deg2rad(df)  # shape (n_listings, 2)\n",
    "earth_radius_km = 6371.0\n",
    "\n",
    "# --- 3. Define radius per amenity (in km) ---\n",
    "amenity_radius_map = {\n",
    "    \"school\": [0.5, 1.0],\n",
    "    \"park\": [0.5, 1.0],\n",
    "    \"supermarket\": [0.5, 1.0],\n",
    "    \"hospital\": [1.0, 2.0],\n",
    "    \"bus_stop\": [0.5, 1.0],\n",
    "    \"tram_stop\": [0.5, 1.0],\n",
    "    \"metro_stop\": [0.5, 1.0],\n",
    "    \"light_rail_stop\": [0.5, 1.0]\n",
    "}\n",
    "\n",
    "# --- 4. Loop over amenities ---\n",
    "amenity_types = amenities_df['amenity_type'].unique()\n",
    "\n",
    "for amenity in amenity_types:\n",
    "    if amenity not in amenity_radius_map:\n",
    "        continue\n",
    "    subset = amenities_df[amenities_df['amenity_type'] == amenity]\n",
    "    amenity_coords_rad = deg2rad(subset)\n",
    "    \n",
    "    # Build BallTree\n",
    "    tree = BallTree(amenity_coords_rad, metric='haversine')\n",
    "    \n",
    "    # Compute counts for each radius\n",
    "    for r_km in amenity_radius_map[amenity]:\n",
    "        r_rad = r_km / earth_radius_km\n",
    "        counts = tree.query_radius(listing_coords_rad, r=r_rad, count_only=True)\n",
    "        col_name = f'count_{amenity}_within_{int(r_km*1000)}m'\n",
    "        df[col_name] = counts\n",
    "\n",
    "# --- 5. Bin counts and ordinal encode ---\n",
    "amenity_count_cols = [col for col in df.columns if col.startswith('count_')]\n",
    "\n",
    "# Ensure numeric\n",
    "for col in amenity_count_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Define bins and ordinal mapping\n",
    "bins = [-1, 0, 2, 5, 10, np.inf]\n",
    "labels = ['0', '1-2', '3-5', '6-10', '10+']\n",
    "ordinal_mapping = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "for col in amenity_count_cols:\n",
    "    bin_col = f'{col}_bin'\n",
    "    ord_col = f'{col}_bin_encoded'\n",
    "    df[bin_col] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)\n",
    "    df[ord_col] = df[bin_col].map(ordinal_mapping)\n",
    "\n",
    "# --- 6. Verify ---\n",
    "print(df[[col for col in df.columns if 'count_' in col]].head())\n",
    "print(df[[col for col in df.columns if 'bin_encoded' in col]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_and_bin_cols = [col for col in df.columns if col.startswith('count_') and not col.endswith('_bin_encoded')]\n",
    "df.drop(columns=raw_and_bin_cols, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Get prepared data ---\n",
    "X_train, X_test, y_train, y_test, X_val, y_val, scaler, feature_encoders = prepare_data(\n",
    "    df=df_clean,\n",
    "    config_path=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_name=\"xgboost_early_stopping\",\n",
    "    use_extended_features=True,\n",
    "    cv=False\n",
    ")\n",
    "\n",
    "# --- 2. Concatenate back to single dataframe ---\n",
    "train_df = X_train.copy()\n",
    "train_df['target'] = y_train\n",
    "\n",
    "val_df = X_val.copy() if X_val is not None else None\n",
    "if val_df is not None:\n",
    "    val_df['target'] = y_val\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# --- 3. Merge new features (amenity counts, distance bins, etc.) ---\n",
    "# Ensure the index or a unique ID is preserved for merge\n",
    "\n",
    "new_feature_cols = [\n",
    "    # Distance bins\n",
    "    'distbin_0–2km',\n",
    "    'distbin_2–5km',\n",
    "    'distbin_5–10km',\n",
    "    'distbin_10–20km',\n",
    "    'distbin_20km+',\n",
    "\n",
    "    # Amenity counts (encoded bins)\n",
    "    'count_school_within_500m_bin_encoded',\n",
    "    'count_school_within_1000m_bin_encoded',\n",
    "    'count_park_within_500m_bin_encoded',\n",
    "    'count_park_within_1000m_bin_encoded',\n",
    "    'count_supermarket_within_500m_bin_encoded',\n",
    "    'count_supermarket_within_1000m_bin_encoded',\n",
    "    'count_bus_stop_within_500m_bin_encoded',\n",
    "    'count_bus_stop_within_1000m_bin_encoded',\n",
    "    'count_tram_stop_within_500m_bin_encoded',\n",
    "    'count_tram_stop_within_1000m_bin_encoded',\n",
    "    'count_metro_stop_within_500m_bin_encoded',\n",
    "    'count_metro_stop_within_1000m_bin_encoded'\n",
    "]\n",
    "\n",
    "train_df = train_df.merge(df[new_feature_cols], left_index=True, right_index=True, how='left')\n",
    "if val_df is not None:\n",
    "    val_df = val_df.merge(df[new_feature_cols], left_index=True, right_index=True, how='left')\n",
    "test_df = test_df.merge(df[new_feature_cols], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# --- Ensure new features are numeric ---\n",
    "for col in new_feature_cols:\n",
    "    if str(train_df[col].dtype) == 'category':\n",
    "        # convert category to int codes\n",
    "        train_df[col] = train_df[col].cat.codes\n",
    "        if val_df is not None:\n",
    "            val_df[col] = val_df[col].cat.codes\n",
    "        test_df[col] = test_df[col].cat.codes\n",
    "    else:\n",
    "        # convert object (string) to numeric\n",
    "        train_df[col] = pd.to_numeric(train_df[col], errors='coerce').fillna(0)\n",
    "        if val_df is not None:\n",
    "            val_df[col] = pd.to_numeric(val_df[col], errors='coerce').fillna(0)\n",
    "        test_df[col] = pd.to_numeric(test_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# --- Split again into X/y ---\n",
    "X_train_final = train_df.drop(columns=['target'])\n",
    "y_train_final = train_df['target']\n",
    "\n",
    "if val_df is not None:\n",
    "    X_val_final = val_df.drop(columns=['target'])\n",
    "    y_val_final = val_df['target']\n",
    "else:\n",
    "    X_val_final, y_val_final = None, None\n",
    "\n",
    "X_test_final = test_df.drop(columns=['target'])\n",
    "y_test_final = test_df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb028cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator(\n",
    "    target_transform=np.log1p, \n",
    "    inverse_transform=np.expm1,\n",
    ")\n",
    "\n",
    "best_xgb_params = study_xgb.best_params\n",
    "\n",
    "trained_xgb, y_train_pred, y_val_pred, y_test_pred, results_xgb = evaluator.evaluate(\n",
    "    model=None,  # not used in XGBoost.train\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train_final,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val_final,\n",
    "    X_test=X_test_final,\n",
    "    y_test=y_test_final,\n",
    "    use_xgb_train=True,\n",
    "    model_params=best_xgb_params,  # <--- crucial\n",
    "    fit_params={\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    ")\n",
    "\n",
    "logger.log_model(\n",
    "    trained_xgb,\n",
    "    \"XGB_Optuna_LogTransformed_with_new_features\",\n",
    "    results_xgb,\n",
    "    use_xgb_train=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd756e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
