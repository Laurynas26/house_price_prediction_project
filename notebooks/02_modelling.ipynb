{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### 1. Imports and Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "mlflow.set_tracking_uri(\"../logs/mlruns\")\n",
    "mlflow.set_experiment(\"house_price_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adjust the path to your project root folder\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(\"..\")\n",
    ")  # from notebooks/ up one level\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.data_loading.data_loading.data_loader import load_data_from_json\n",
    "from src.data_loading.preprocessing.preprocessing import preprocess_df\n",
    "\n",
    "\n",
    "df_raw = load_data_from_json(\"../data/parsed_json/*.json\")\n",
    "df_clean = preprocess_df(df_raw)\n",
    "df_clean\n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### 2. Load and prep data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year(year):\n",
    "    if isinstance(year, str):\n",
    "        if year.startswith(\"Voor\"):  # e.g., \"Voor 1906\"\n",
    "            return int(year.split()[-1]) - 1  # use 1905\n",
    "        elif year.startswith(\"Na\"):  # e.g., \"Na 2020\"\n",
    "            return int(year.split()[-1]) + 1  # use 2021\n",
    "        elif year.isdigit():\n",
    "            return int(year)\n",
    "        else:\n",
    "            return None  # invalid string\n",
    "    elif isinstance(year, (int, float)):\n",
    "        return int(year)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"year_of_construction\"] = df[\"year_of_construction\"].apply(clean_year)\n",
    "df[\"year_of_construction\"] = df[\"year_of_construction\"].fillna(\n",
    "    df[\"year_of_construction\"].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"bedrooms\", \"nr_rooms\", \"bathrooms\", \"toilets\"]\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"price_num\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features = [\n",
    "    \"size_num\",\n",
    "    \"bedrooms\",\n",
    "    \"year_of_construction\",\n",
    "    \"nr_rooms\",\n",
    "    \"bathrooms\",\n",
    "    \"toilets\",\n",
    "    \"contribution_vve_num\",\n",
    "    \"external_storage_num\",\n",
    "    \"inhabitants_in_neighborhood\",\n",
    "    \"families_with_children_pct\",\n",
    "    \"price_per_m2_neighborhood\",\n",
    "]\n",
    "target = \"price_num\"\n",
    "\n",
    "X = df[linear_features].replace(\"N/A\", np.nan).fillna(0)\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### 3. Code for evaluating and logging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, metrics=None, fit_params=None):\n",
    "    \"\"\"\n",
    "    Fit model, predict, and return evaluation metrics.\n",
    "    \"\"\"\n",
    "    if fit_params is None:\n",
    "         fit_params={}\n",
    "    model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    results = {}\n",
    "    if metrics is None:\n",
    "        metrics = {\n",
    "            \"rmse\": lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)),\n",
    "            \"mea\": lambda y, y_pred: mean_absolute_error(y, y_pred),\n",
    "            \"r2\": r2_score,\n",
    "        }\n",
    "\n",
    "    for name, func in metrics.items():\n",
    "        results[f\"train_{name}\"] = func(y_train, y_train_pred)\n",
    "        results[f\"test_{name}\"] = func(y_test, y_test_pred)\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "def log_to_mlflow(model, model_name, results):\n",
    "    \"\"\"\n",
    "    Log model and metrics to MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "            mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "            mlflow.log_metrics(results)\n",
    "            if hasattr(model, \"get_params\"):\n",
    "                mlflow.log_params(model.get_params())   \n",
    "            print(f\"{model_name} -> {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### 4. Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, lr_results = evaluate_model(\n",
    "    LinearRegression(),\n",
    "    X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "log_to_mlflow(lr, \"Linear_Regression\", lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### 5. Random Forest Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \"size_num\",\n",
    "    \"bedrooms\",\n",
    "    \"energy_label\",\n",
    "    \"year_of_construction\",\n",
    "    \"nr_rooms\",\n",
    "    \"bathrooms\",\n",
    "    \"toilets\",\n",
    "    \"contribution_vve_num\",\n",
    "    \"external_storage_num\",\n",
    "    \"has_mechanische_ventilatie\",\n",
    "    \"has_tv_kabel\",\n",
    "    \"has_lift\",\n",
    "    \"has_natuurlijke_ventilatie\",\n",
    "    \"has_n/a\",\n",
    "    \"has_schuifpui\",\n",
    "    \"has_glasvezelkabel\",\n",
    "    \"has_frans_balkon\",\n",
    "    \"has_buitenzonwering\",\n",
    "    \"has_zonnepanelen\",\n",
    "    \"has_airconditioning\",\n",
    "    \"has_balansventilatie\",\n",
    "    \"has_dakraam\",\n",
    "    \"has_alarminstallatie\",\n",
    "    \"has_domotica\",\n",
    "    \"has_rookkanaal\",\n",
    "    \"has_elektra\",\n",
    "    \"has_sauna\",\n",
    "    \"has_zonnecollectoren\",\n",
    "    \"has_cctv\",\n",
    "    \"has_rolluiken\",\n",
    "    \"has_stromend_water\",\n",
    "    \"has_satellietschotel\",\n",
    "    \"num_facilities\",\n",
    "    \"inhabitants_in_neighborhood\",\n",
    "    \"families_with_children_pct\",\n",
    "    \"price_per_m2_neighborhood\",\n",
    "]\n",
    "\n",
    "X = df[all_features].replace(\"N/A\", np.nan).fillna(0)\n",
    "\n",
    "X[\"energy_label\"] = X[\"energy_label\"].replace({0: \"G\"})\n",
    "energy_order = [\n",
    "    \"G\",\n",
    "    \"F\",\n",
    "    \"E\",\n",
    "    \"D\",\n",
    "    \"C\",\n",
    "    \"B\",\n",
    "    \"A\",\n",
    "    \"A+\",\n",
    "    \"A++\",\n",
    "    \"A+++\",\n",
    "    \"A++++\",\n",
    "]\n",
    "encoder = OrdinalEncoder(categories=[energy_order])\n",
    "X[\"energy_label_encoded\"] = encoder.fit_transform(X[[\"energy_label\"]])\n",
    "X = X.drop(columns=[\"energy_label\"])\n",
    "\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.energy_label_encoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, rf_results = evaluate_model(\n",
    "    RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42),\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "log_to_mlflow(rf, \"Random_Forest_Regression\", rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### 6. XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42 \n",
    ")\n",
    "xgb_model, results = evaluate_model(\n",
    "    xgb_model,\n",
    "    X_train, y_train, X_test, y_test,\n",
    ")\n",
    "log_to_mlflow(xgb_model, \"XGBoost_Regression\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### 7. XGBoost with early stopping and more tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Convert to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.05,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtest, \"eval\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = xgb_model.predict(dtrain)\n",
    "y_test_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae  = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, train_R²: {train_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, test_R²: {test_r2:.3f}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost_Regression\"):\n",
    "    # Log model\n",
    "    mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgb_model\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)    \n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### 7. Compare models using MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"house_price_prediction\" \n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_of_interest = [\"metrics.train_rmse\", \"metrics.test_rmse\", \"metrics.train_r2\", \"metrics.test_r2\"]\n",
    "comparison_df = runs_df[[\"run_id\", \"tags.mlflow.runName\"] + metrics_of_interest]\n",
    "\n",
    "comparison_df.sort_values(\"metrics.test_r2\", ascending=False, inplace=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = comparison_df.sort_values(\"metrics.test_r2\", ascending=False).iloc[0]\n",
    "print(\"Best model based on test R²:\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### 8. Hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"seed\": 42,\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtest, \"eval\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(dtrain)\n",
    "    y_test_pred = model.predict(dtest)\n",
    "\n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.2f}, Test RMSE: {test_rmse:.2f}, Train R²: {train_r2:.3f}, Test R²: {test_r2:.3f}\")\n",
    "\n",
    "\n",
    "    # Optuna only optimizes one metric, here test RMSE\n",
    "    return test_rmse\n",
    "\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    \n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_xgb = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=30)  \n",
    "\n",
    "print(\"Best XGBoost params:\", study_xgb.best_params)\n",
    "print(\"Best XGBoost Test RMSE:\", study_xgb.best_value)\n",
    "\n",
    "# Run Optuna for RandomForest\n",
    "study_rf = optuna.create_study(direction=\"minimize\")\n",
    "study_rf.optimize(objective_rf, n_trials=30)\n",
    "\n",
    "print(\"Best RF params:\", study_rf.best_params)\n",
    "print(\"Best RF Test RMSE:\", study_rf.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### 9. RF and Xgboost with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestRegressor(**study_rf.best_params)\n",
    "best_rf, results_rf = evaluate_model(best_rf, X_train, y_train, X_test, y_test)\n",
    "log_to_mlflow(best_rf, \"RF_Optuna\", results_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xgb_dmatrix(params, dtrain, dtest, run_name=\"XGBoost_Regression\", num_boost_round=500, early_stopping_rounds=50):\n",
    "    \"\"\"\n",
    "    Train and evaluate an XGBoost model using DMatrix + xgb.train,\n",
    "    log metrics and model to MLflow.\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    xgb_model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dtest, \"eval\")],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = xgb_model.predict(dtrain)\n",
    "    y_test_pred = xgb_model.predict(dtest)\n",
    "\n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae  = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae  = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    results = {\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2,\n",
    "    }\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, train_R²: {train_r2:.3f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, test_R²: {test_r2:.3f}\")\n",
    "\n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgb_model\")\n",
    "        mlflow.log_metrics(results)\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "    return xgb_model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study_xgb.best_params\n",
    "best_params.update({\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"seed\": 42,\n",
    "    \"tree_method\": \"hist\"\n",
    "})\n",
    "best_xgb, results_xgb = evaluate_xgb_dmatrix(best_params, dtrain, dtest, run_name=\"XGB_Optuna\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### 10. Let's see how outliers skew RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot target distribution\n",
    "sns.boxplot(y=y_test)\n",
    "plt.show()\n",
    "\n",
    "# Optional: scatter of predictions vs true values\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Summary stats\n",
    "print(\"Residuals summary:\")\n",
    "print(\"Min:\", np.min(residuals))\n",
    "print(\"Max:\", np.max(residuals))\n",
    "print(\"Median:\", np.median(residuals))\n",
    "print(\"Mean:\", np.mean(residuals))\n",
    "print(\"Std:\", np.std(residuals))\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.title(\"Residuals Distribution\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MAE: {train_mae:.2f}\")\n",
    "print(f\"Test MAE:  {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "So outliers are skewing the RMSE statistic quite heavily. Hence, I will log transform the target and dot he same analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train)  # log(1 + price)\n",
    "y_test_log  = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_log(model, X_train, y_train_log, X_test, y_test_log):\n",
    "    \"\"\"Fit model on log-transformed target and compute predictions on original scale.\"\"\"\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train_log)\n",
    "    \n",
    "    # Predict and back-transform\n",
    "    y_train_pred = np.expm1(model.predict(X_train))\n",
    "    y_test_pred  = np.expm1(model.predict(X_test))\n",
    "    \n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse  = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae  = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae   = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2   = r2_score(y_train, y_train_pred)\n",
    "    test_r2    = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    results = {\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2\n",
    "    }\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_test - y_test_pred\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(residuals, bins=50)\n",
    "    plt.title(\"Residuals Distribution\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model, results, y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf, rf_results, rf_pred = evaluate_model_log(rf, X_train, y_train_log, X_test, y_test_log)\n",
    "log_to_mlflow(rf, \"RF_LogTransform\", rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xgb_dmatrix_log(params, dtrain, dtest, y_train, y_test, run_name=\"XGBoost_Regression_Log\", \n",
    "                             num_boost_round=500, early_stopping_rounds=50):\n",
    "    \"\"\"\n",
    "    Train and evaluate an XGBoost model using DMatrix + xgb.train on log-transformed target,\n",
    "    log metrics and model to MLflow.\n",
    "    \n",
    "    y_train, y_test: original (untransformed) target arrays, used for computing metrics on original scale.\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    xgb_model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dtest, \"eval\")],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predictions on log scale\n",
    "    y_train_pred_log = xgb_model.predict(dtrain)\n",
    "    y_test_pred_log  = xgb_model.predict(dtest)\n",
    "\n",
    "    # Back-transform to original scale\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_test_pred  = np.expm1(y_test_pred_log)\n",
    "\n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse  = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae  = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae   = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2   = r2_score(y_train, y_train_pred)\n",
    "    test_r2    = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    results = {\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2\n",
    "    }\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, R²: {train_r2:.3f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, R²: {test_r2:.3f}\")\n",
    "\n",
    "    # Residuals\n",
    "    residuals = y_test - y_test_pred\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(residuals, bins=50)\n",
    "    plt.title(\"Residuals Distribution\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgb_model\")\n",
    "        mlflow.log_metrics(results)\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "    return xgb_model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform y for DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=np.log1p(y_train))\n",
    "dtest  = xgb.DMatrix(X_test, label=np.log1p(y_test))\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.05,\n",
    "    \"seed\": 42,\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "xgb_model_log, results_log = evaluate_xgb_dmatrix_log(params, dtrain, dtest, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Define extreme outliers: e.g., top 5% of absolute residuals\n",
    "threshold = np.percentile(np.abs(residuals), 95)\n",
    "outliers_mask = np.abs(residuals) >= threshold\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot non-outliers\n",
    "plt.scatter(y_test[~outliers_mask], y_test_pred[~outliers_mask], alpha=0.5, label=\"Normal listings\")\n",
    "\n",
    "# Highlight extreme residuals\n",
    "plt.scatter(y_test[outliers_mask], y_test_pred[outliers_mask], color='red', label=\"Extreme listings\")\n",
    "\n",
    "# Diagonal line (perfect prediction)\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "plt.plot([0, max_val], [0, max_val], color='black', linestyle='--', label=\"Perfect prediction\")\n",
    "\n",
    "plt.xlabel(\"Actual Price (€)\")\n",
    "plt.ylabel(\"Predicted Price (€)\")\n",
    "plt.title(\"Predicted vs Actual Prices with Extreme Listings Highlighted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "This is a clear visualization to see how predictions behave across the entire range and highlight the extreme listings that inflate RMSE. Large RMSE is not a deal breaker since:\n",
    "\n",
    "**Statistical justification**\n",
    "Skewed distribution: My dataset has a few extremely expensive houses that are far from the mean. RMSE is sensitive to large errors because it squares residuals, so these few points dominate the metric.\n",
    "\n",
    "MAE is more robust: By reporting MAE alongside RMSE, I show the typical prediction error for most listings, which is a fairer assessment of model performance.\n",
    "\n",
    "Log-transform mitigates skew: Training on log1p(y) reduces the influence of outliers and stabilizes variance, producing a more reliable model for the bulk of the data.\n",
    "\n",
    "**Practical/business justification**\n",
    "\n",
    "The extreme listings (multi-million € homes) are rare. The model performs well on 99% of listings, which is what matters for most users or business decisions.\n",
    "\n",
    "Trying to perfectly predict the top 1–5% of luxury listings would:\n",
    "\n",
    "Require specialized models or additional data\n",
    "\n",
    "Complicate the pipeline\n",
    "\n",
    "Increase overfitting risk\n",
    "\n",
    "Reporting MAE and residual distributions communicates clearly that errors on extreme listings exist, but are expected and do not invalidate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### 11. New approach and moving with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "# XGBoost objective for Optuna (optimize MAE)\n",
    "def objective_xgb(trial, X_train, y_train, X_test, y_test):\n",
    "    # Log-transform target\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    y_test_log  = np.log1p(y_test)\n",
    "\n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_log)\n",
    "    dtest  = xgb.DMatrix(X_test, label=y_test_log)\n",
    "\n",
    "    # Hyperparameters\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5.0),\n",
    "        \"seed\": 42,\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    pruning_callback = XGBoostPruningCallback(trial, \"eval-mae\")\n",
    "\n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtest, \"eval\")],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False,\n",
    "        callbacks=[pruning_callback]\n",
    "    )\n",
    "\n",
    "    # Back-transform predictions\n",
    "    y_train_pred = np.expm1(model.predict(dtrain))\n",
    "    y_test_pred  = np.expm1(model.predict(dtest))\n",
    "\n",
    "    # Compute MAE on original scale\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae  = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Train MAE: {train_mae:.2f}, Test MAE: {test_mae:.2f}\")\n",
    "\n",
    "    return test_mae  # Optuna optimizes this\n",
    "\n",
    "\n",
    "# Random Forest objective for Optuna (optimize MAE)\n",
    "def objective_rf(trial, X_train, y_train, X_test, y_test):\n",
    "    # Log-transform target\n",
    "    y_train_log = np.log1p(y_train)\n",
    "\n",
    "    # Hyperparameters\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train_log)\n",
    "\n",
    "    # Back-transform predictions\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return test_mae  # Optuna optimizes this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna for XGBoost\n",
    "sampler = optuna.samplers.TPESampler(seed=42)  \n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "\n",
    "study_xgb = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "study_xgb.optimize(\n",
    "    lambda trial: objective_xgb(trial, X_train, y_train, X_test, y_test),\n",
    "    n_trials=200\n",
    ")\n",
    "\n",
    "print(\"Best XGBoost params:\", study_xgb.best_params)\n",
    "print(\"Best XGBoost Test MAE:\", study_xgb.best_value)\n",
    "\n",
    "# Run Optuna for Random Forest\n",
    "study_rf = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "study_rf.optimize(\n",
    "    lambda trial: objective_rf(trial, X_train, y_train, X_test, y_test),\n",
    "    n_trials=100\n",
    ")\n",
    "\n",
    "print(\"Best RF params:\", study_rf.best_params)\n",
    "print(\"Best RF Test MAE:\", study_rf.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xgb_dmatrix(params, dtrain, dtest, y_train_orig, y_test_orig, \n",
    "                         run_name=\"XGBoost_Regression\", num_boost_round=500, early_stopping_rounds=50):\n",
    "    \"\"\"\n",
    "    Train and evaluate an XGBoost model using DMatrix + xgb.train,\n",
    "    with support for log-transformed targets.\n",
    "    Logs RMSE, MAE, R², and hyperparameters to MLflow.\n",
    "    \n",
    "    Parameters:\n",
    "    - params: dict of XGBoost parameters\n",
    "    - dtrain, dtest: xgb.DMatrix (log-transformed labels)\n",
    "    - y_train_orig, y_test_orig: original target values (not log-transformed)\n",
    "    - run_name: MLflow run name\n",
    "    - num_boost_round: maximum number of boosting iterations\n",
    "    - early_stopping_rounds: rounds for early stopping\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    xgb_model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dtest, \"eval\")],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predict and back-transform to original scale\n",
    "    y_train_pred = np.expm1(xgb_model.predict(dtrain))\n",
    "    y_test_pred = np.expm1(xgb_model.predict(dtest))\n",
    "\n",
    "    # Compute metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_orig, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_orig, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train_orig, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test_orig, y_test_pred)\n",
    "    train_r2 = r2_score(y_train_orig, y_train_pred)\n",
    "    test_r2 = r2_score(y_test_orig, y_test_pred)\n",
    "\n",
    "    results = {\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2,\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Train RMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, R²: {train_r2:.3f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, R²: {test_r2:.3f}\")\n",
    "\n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgb_model\")\n",
    "        mlflow.log_metrics(results)\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "    return xgb_model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log1p targets\n",
    "dtrain = xgb.DMatrix(X_train, label=np.log1p(y_train))\n",
    "dtest = xgb.DMatrix(X_test, label=np.log1p(y_test))\n",
    "\n",
    "best_params = study_xgb.best_params\n",
    "best_params.update({\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"seed\": 42,\n",
    "    \"tree_method\": \"hist\"\n",
    "})\n",
    "\n",
    "best_xgb, results_xgb = evaluate_xgb_dmatrix(\n",
    "    best_params, dtrain, dtest, y_train_orig=y_train, y_test_orig=y_test, \n",
    "    run_name=\"XGB_Optuna_LogTransformed\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare log-transformed targets\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# Refit best RF on log targets and evaluate\n",
    "best_rf = RandomForestRegressor(**study_rf.best_params)\n",
    "best_rf, results_rf, y_test_pred = evaluate_model_log(best_rf, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "# Log to MLflow\n",
    "log_to_mlflow(best_rf, \"RF_LogTransform_Optuna\", results_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"house_price_prediction\" \n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "metrics_of_interest = [\"metrics.train_rmse\", \"metrics.test_rmse\", \"metrics.train_r2\", \"metrics.test_r2\", \"metrics.train_mae\", \"metrics.test_mae\"]\n",
    "comparison_df = runs_df[[\"run_id\", \"tags.mlflow.runName\"] + metrics_of_interest]\n",
    "\n",
    "comparison_df.sort_values(\"metrics.test_mae\", ascending=True, inplace=True)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = comparison_df.sort_values(\"metrics.test_mae\", ascending=True).iloc[0]\n",
    "print(\"Best model based on test MAE:\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
