{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc88793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "tracking_uri = \"../logs/mlruns\"\n",
    "os.makedirs(os.path.join(tracking_uri, \".trash\"), exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"house_price_prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Adjust the path to your project root folder\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(\"..\")\n",
    ")  # from notebooks/ up one level\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.data_loading.data_loading.data_loader import load_data_from_json\n",
    "from src.data_loading.preprocessing.preprocessing import preprocess_df\n",
    "from src.data_loading.preprocessing.imputation import impute_missing_values\n",
    "\n",
    "\n",
    "# go two levels up from notebook dir -> project root\n",
    "ROOT = (\n",
    "    Path(__file__).resolve().parents[2]\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parents[1]\n",
    ")\n",
    "CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"preprocessing_config.yaml\"\n",
    ")\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "df_raw = load_data_from_json(\"../data/parsed_json/*.json\")\n",
    "df_clean = preprocess_df(\n",
    "    df_raw,\n",
    "    drop_raw=CONFIG[\"preprocessing\"][\"drop_raw\"],\n",
    "    numeric_cols=CONFIG[\"preprocessing\"][\"numeric_cols\"],\n",
    ")\n",
    "df_clean = impute_missing_values(\n",
    "    df_clean, CONFIG[\"preprocessing\"][\"imputation\"]\n",
    ")\n",
    "# Drop price_num NaNs for the training of the model\n",
    "df_clean = df_clean[df_clean[\"price_num\"].notna()]\n",
    "df_clean.drop(columns=[\"living_area\"], inplace=True)\n",
    "\n",
    "\n",
    "# df_clean = df_clean[:100] \n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff54eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Adjust ROOT to your project root ---\n",
    "ROOT = Path(\"C:/Users/LaurynasBaltrusaitis/OneDrive - Adaptfy BV/Desktop/Education/git_personal_repos\")\n",
    "\n",
    "CONFIG_PATH = ROOT / \"house_price_prediction_project\" / \"config\" / \"model_config.yaml\"\n",
    "\n",
    "from src.features.data_prep_for_modelling.data_preparation import load_geo_config\n",
    "from src.features.feature_engineering.location_feature_enrichment import load_cache\n",
    "\n",
    "# 1️⃣ Test load_geo_config\n",
    "geo_cache_file, amenities_df, amenity_radius_map = load_geo_config(CONFIG_PATH)\n",
    "print(\"Geo cache file:\", geo_cache_file)\n",
    "print(\"Amenities df:\", amenities_df.shape if amenities_df is not None else None)\n",
    "print(\"Amenity radius map:\", amenity_radius_map)\n",
    "\n",
    "# 2️⃣ Test load_cache\n",
    "lat_lon_cache = load_cache(geo_cache_file)\n",
    "print(\"Number of addresses in cache:\", len(lat_lon_cache))\n",
    "print(\"Sample from cache:\", list(lat_lon_cache.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cache_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ad5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.data_prep_for_modelling.data_preparation import prepare_data\n",
    "\n",
    "FEATURES_CONFIG_PATH = (\n",
    "    ROOT / \"house_price_prediction_project\" / \"config\" / \"model_config.yaml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1263882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.evaluate import ModelEvaluator\n",
    "from src.model.mlflow_logger import MLFlowLogger\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "logger = MLFlowLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../data/df_with_lat_lon_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b73fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "from src.features.data_prep_for_modelling.data_preparation import prepare_data_from_config\n",
    "\n",
    "\n",
    "config_path = FEATURES_AND_MODEL_CONFIG_PATH  # e.g., ROOT / \"config/model_config.yaml\"\n",
    "from src.features.feature_engineering.location_feature_enrichment import load_cache\n",
    "\n",
    "\n",
    "# Call the wrapper\n",
    "X_train, X_test, y_train, y_test, X_val, y_val, scaler, meta = prepare_data_from_config(\n",
    "    df=df_clean,\n",
    "    config_path=config_path,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng_geoloc_exp\"\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape if X_val is not None else None)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.copy()\n",
    "X_test_final = X_test.copy()\n",
    "X_val_final = X_val.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train_final.shape)\n",
    "print(\"validation shape:\", X_val_final.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with log-transform\n",
    "\n",
    "from functools import partial\n",
    "from src.model.objectives_optuna import unified_objective\n",
    "\n",
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "study_xgb = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "objective_xgb_partial = partial(\n",
    "    unified_objective,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng_geoloc_exp\",\n",
    "    df=df_clean,\n",
    "    features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    use_log=True,  \n",
    "    n_splits=5,\n",
    "    use_extended_features=True,\n",
    "    use_geo_amenities=True,\n",
    ")\n",
    "study_xgb.optimize(objective_xgb_partial, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"postal_code_clean\", \"lat\", \"lon\"]  # add any other object columns if needed\n",
    "\n",
    "X_train_final = X_train_final.drop(columns=[c for c in cols_to_drop if c in X_train_final.columns])\n",
    "X_val_final   = X_val_final.drop(columns=[c for c in cols_to_drop if c in X_val_final.columns])\n",
    "X_test_final  = X_test_final.drop(columns=[c for c in cols_to_drop if c in X_test_final.columns])\n",
    "\n",
    "# --- XGBoost ---\n",
    "evaluator = ModelEvaluator(target_transform=np.log1p, inverse_transform=np.expm1)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "\n",
    "trained_xgb, y_train_pred, y_val_pred, y_test_pred, results_xgb = evaluator.evaluate(\n",
    "    model=None,  # not used in XGBoost.train\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_final,\n",
    "    y_test=y_test,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    use_xgb_train=True,\n",
    "    model_params=best_xgb_params,  # <--- crucial\n",
    "    fit_params={\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    ")\n",
    "logger.log_model(trained_xgb, \"XGB_Optuna_LogTransformed_feature_eng\", results_xgb, use_xgb_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from src.model.cv_helpers import prepare_base_data, prepare_fold_features\n",
    "from src.model.evaluate import ModelEvaluator\n",
    "\n",
    "def final_cv_evaluation(\n",
    "    df,\n",
    "    features_config,\n",
    "    model_name,\n",
    "    best_params,\n",
    "    use_log=True,\n",
    "    n_splits=5,\n",
    "    use_geo_amenities=True,\n",
    "    enable_cache_save=False,\n",
    "    fit_params=None,\n",
    "):\n",
    "    # Prepare data (same as in unified_objective)\n",
    "    X_full, y_full = prepare_base_data(\n",
    "        df, features_config, model_name, extended_fe=True\n",
    "    )\n",
    "\n",
    "    target_transform = np.log1p if use_log else None\n",
    "    inverse_transform = np.expm1 if use_log else None\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_metrics = []\n",
    "    oof_preds = pd.Series(index=X_full.index, dtype=float)\n",
    "    evaluator = ModelEvaluator(target_transform=target_transform,\n",
    "                               inverse_transform=inverse_transform)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "        X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "        y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "\n",
    "        # fold-wise feature engineering (same path as unified_objective)\n",
    "        X_train, X_val, meta, fold_encoders = prepare_fold_features(\n",
    "            X_train,\n",
    "            X_val,\n",
    "            features_config=features_config if use_geo_amenities else None,\n",
    "            use_extended_features=True,\n",
    "            enable_cache_save=enable_cache_save,\n",
    "        )\n",
    "\n",
    "        # evaluate using your evaluator (use X_val as \"test\" for the fold)\n",
    "        trained_model, y_train_pred, y_val_pred, _, results = evaluator.evaluate(\n",
    "            model=None,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_val,\n",
    "            y_test=y_val,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            use_xgb_train=True if \"xgb\" in model_name.lower() else False,\n",
    "            model_params=best_params,\n",
    "            fit_params=fit_params or {\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    "        )\n",
    "\n",
    "        # Collect fold metrics (results should contain val_rmse, val_mae, val_mape)\n",
    "        fold_metrics.append({\n",
    "            \"fold\": fold,\n",
    "            \"val_rmse\": results.get(\"val_rmse\"),\n",
    "            \"val_mae\": results.get(\"val_mae\"),\n",
    "            \"val_mape\": results.get(\"val_mape\"),\n",
    "            \"val_r2\": results.get(\"val_r2\"),\n",
    "        })\n",
    "\n",
    "        # Save OOF preds (inverse_transform already applied by evaluator if that's its contract)\n",
    "        oof_preds.iloc[val_idx] = y_val_pred  # ensure y_val_pred is inverse-transformed\n",
    "\n",
    "    # Aggregate\n",
    "    df_metrics = pd.DataFrame(fold_metrics).set_index(\"fold\")\n",
    "    agg = df_metrics.agg([\"mean\", \"std\"]).T\n",
    "\n",
    "    return {\n",
    "        \"fold_metrics\": df_metrics,\n",
    "        \"agg_metrics\": agg,\n",
    "        \"oof_preds\": oof_preds,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa53ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from src.model.cv_helpers import prepare_base_data, prepare_fold_features\n",
    "\n",
    "# Number of folds (should match your CV setup)\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Prepare base data (features + target)\n",
    "X_full, y_full = prepare_base_data(df_clean, FEATURES_AND_MODEL_CONFIG_PATH, \"xgboost_early_stopping_optuna_feature_eng_geoloc_exp\")\n",
    "\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "fold_importances = {imp_type: [] for imp_type in importance_types}\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "    X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "    y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "\n",
    "    # Prepare fold-wise features (ensure extended feature engineering matches training)\n",
    "    X_train_fold, X_val_fold, _, _ = prepare_fold_features(X_train, X_val, features_config=FEATURES_AND_MODEL_CONFIG_PATH, use_extended_features=True, enable_cache_save=False)\n",
    "\n",
    "    cols_to_drop = [\"size_num\", \"lat\", \"lon\"]  # or just one of them\n",
    "    X_train_fold = X_train_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    X_val_fold = X_val_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Train XGBoost on this fold using best params\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=np.log1p(y_train))\n",
    "    dval = xgb.DMatrix(X_val_fold, label=np.log1p(y_val))\n",
    "    \n",
    "    model_fold = xgb.train(\n",
    "        params=best_xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Collect importance for each type\n",
    "    for imp_type in importance_types:\n",
    "        imp_dict = model_fold.get_score(importance_type=imp_type)\n",
    "        df_imp = pd.DataFrame.from_dict(imp_dict, orient=\"index\", columns=[imp_type])\n",
    "        df_imp.index.name = \"feature\"\n",
    "        fold_importances[imp_type].append(df_imp)\n",
    "\n",
    "# --- Aggregate across folds ---\n",
    "agg_importances = {}\n",
    "for imp_type, dfs in fold_importances.items():\n",
    "    # Combine all folds into a single dataframe\n",
    "    df_all = pd.concat(dfs, axis=1).fillna(0)\n",
    "    df_all[\"mean\"] = df_all.mean(axis=1)\n",
    "    df_all = df_all.sort_values(by=\"mean\", ascending=False)\n",
    "    agg_importances[imp_type] = df_all\n",
    "    print(f\"\\nTop 10 features by mean {imp_type} across folds:\")\n",
    "    print(df_all[\"mean\"].head(100))\n",
    "    \n",
    "    # Plot top 20\n",
    "    df_all[\"mean\"].head(200).plot.barh(figsize=(10,6), title=f\"Top 20 features by mean {imp_type} across folds\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "fold_shap_values = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "    X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "    y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "    \n",
    "    # Prepare fold-wise features\n",
    "    X_train_fold, X_val_fold, _, _ = prepare_fold_features(X_train, X_val, features_config=FEATURES_AND_MODEL_CONFIG_PATH, use_extended_features=True, enable_cache_save=False)\n",
    "    cols_to_drop = [\"size_num\", \"lat\", \"lon\"] \n",
    "    X_train_fold = X_train_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    X_val_fold = X_val_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Train XGBoost Booster\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=np.log1p(y_train))\n",
    "    dval = xgb.DMatrix(X_val_fold, label=np.log1p(y_val))\n",
    "    \n",
    "    model_fold = xgb.train(\n",
    "        params=best_xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # SHAP for Booster\n",
    "    explainer = shap.TreeExplainer(model_fold)\n",
    "    shap_values = explainer.shap_values(dval)  # DMatrix\n",
    "    \n",
    "    # Store mean absolute SHAP values per feature for this fold\n",
    "    fold_shap_values.append(pd.DataFrame({\n",
    "        \"feature\": X_val_fold.columns,\n",
    "        \"mean_abs_shap\": np.abs(shap_values).mean(axis=0)\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Combine all folds\n",
    "df_shap_all = pd.concat(fold_shap_values)\n",
    "\n",
    "# Group by feature and compute mean across folds\n",
    "agg_shap = df_shap_all.groupby(\"feature\")[\"mean_abs_shap\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by mean absolute SHAP value across folds:\")\n",
    "print(agg_shap.head(10))\n",
    "\n",
    "# Prepare data\n",
    "top_features = agg_shap.head(10).sort_values()\n",
    "features = top_features.index\n",
    "shap_values = top_features.values\n",
    "models = np.arange(len(features))\n",
    "bar_width = 0.6\n",
    "color = 'skyblue'\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Horizontal bar plot\n",
    "ax.barh(models, shap_values, height=bar_width, color=color)\n",
    "\n",
    "# Y-axis labels\n",
    "ax.set_yticks(models)\n",
    "ax.set_yticklabels(features)\n",
    "ax.invert_yaxis()  # highest SHAP on top\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Mean Absolute SHAP Value\")\n",
    "ax.set_title(\"Top 10 Features by SHAP Value\")\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(shap_values):\n",
    "    ax.text(val + 0.01 * max(shap_values), i, f\"{val:.3f}\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all folds\n",
    "df_shap_all = pd.concat(fold_shap_values)\n",
    "\n",
    "# Group by feature and compute mean across folds\n",
    "agg_shap = df_shap_all.groupby(\"feature\")[\"mean_abs_shap\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features by mean absolute SHAP value across folds:\")\n",
    "print(agg_shap.head(10))\n",
    "\n",
    "# Plot\n",
    "agg_shap.head(10).sort_values().plot.barh(figsize=(10,6), title=\"Top 10 features by SHAP value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "def shap_to_euros(model, X, target_transform_inverse=np.expm1):\n",
    "    \"\"\"\n",
    "    Compute SHAP values in € for a trained XGBoost model with log1p target.\n",
    "    \"\"\"\n",
    "    # Predict log1p(price)\n",
    "    dmatrix = xgb.DMatrix(X)\n",
    "    pred_log = model.predict(dmatrix)\n",
    "\n",
    "    # Get full SHAP values aligned with all columns\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_array = explainer.shap_values(X)  # (n_samples, n_features)\n",
    "\n",
    "    # Sanity check\n",
    "    if shap_values_array.shape[1] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Mismatch: SHAP shape {shap_values_array.shape[1]} vs X {X.shape[1]}\"\n",
    "        )\n",
    "\n",
    "    # Convert SHAP deltas to € scale\n",
    "    price_contrib = np.expm1(pred_log[:, None] + shap_values_array) - np.expm1(pred_log[:, None])\n",
    "    df_shap_euros = pd.DataFrame(price_contrib, columns=X.columns)\n",
    "\n",
    "    mean_abs_euros = df_shap_euros.abs().mean().sort_values(ascending=False)\n",
    "    return mean_abs_euros\n",
    "\n",
    "# Usage:\n",
    "mean_abs_shap_euros = shap_to_euros(model_fold, X_val_fold)\n",
    "print(mean_abs_shap_euros.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b321d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Top 10 SHAP-Euro contributions\n",
    "top_features = pd.Series({\n",
    "    \"log_size_num\": 349033.09,\n",
    "    \"price_per_m2_neighborhood\": 77572.78,\n",
    "    \"outdoor_area_ratio\": 19810.93,\n",
    "    \"luxury_x_price_m2\": 12859.01,\n",
    "    \"size_per_luxury\": 12562.23,\n",
    "    \"dist_to_center_bin_encoded\": 12214.45,\n",
    "    \"energy_label_encoded\": 11375.25,\n",
    "    \"ownership_type_Other\": 10560.33,\n",
    "    \"luxury_x_size\": 8401.32,\n",
    "    \"has_mechanische_ventilatie\": 7442.82\n",
    "}).sort_values()\n",
    "\n",
    "features = top_features.index\n",
    "shap_values = top_features.values\n",
    "models = np.arange(len(features))\n",
    "bar_width = 0.6\n",
    "color = 'skyblue'\n",
    "\n",
    "# Friendly feature names\n",
    "feature_rename = {\n",
    "    \"log_size_num\": \"Property Size (log m²)\",\n",
    "    \"price_per_m2_neighborhood\": \"Price per m² by Neighborhood\",\n",
    "    \"outdoor_area_ratio\": \"Outdoor Area Ratio\",\n",
    "    \"luxury_x_price_m2\": \"Luxury × Price per m²\",\n",
    "    \"size_per_luxury\": \"Size per Luxury Feature\",\n",
    "    \"dist_to_center_bin_encoded\": \"Distance to Center (binned)\",\n",
    "    \"energy_label_encoded\": \"Energy Label\",\n",
    "    \"ownership_type_Other\": \"Other Ownership Type\",\n",
    "    \"luxury_x_size\": \"Luxury × Size\",\n",
    "    \"has_mechanische_ventilatie\": \"Mechanical Ventilation\"\n",
    "}\n",
    "\n",
    "# Notes dictionary\n",
    "notes_dict = {\n",
    "    \"log_size_num\": \"Largest driver of price: bigger properties dominate overall.\",\n",
    "    \"price_per_m2_neighborhood\": \"Captures local market variation and neighborhood pricing.\",\n",
    "    \"outdoor_area_ratio\": \"More outdoor space moderately increases value.\",\n",
    "    \"luxury_x_price_m2\": \"Luxury features amplify local per-m² price.\",\n",
    "    \"size_per_luxury\": \"Reflects amenity density: smaller values indicate more luxury features per m².\",\n",
    "    \"dist_to_center_bin_encoded\": \"Central locations increase price non-linearly.\",\n",
    "    \"energy_label_encoded\": \"Better energy label adds some value.\",\n",
    "    \"ownership_type_Other\": \"Non-standard ownership has moderate effect.\",\n",
    "    \"luxury_x_size\": \"Luxury impact scales with property size.\",\n",
    "    \"has_mechanische_ventilatie\": \"Mechanical ventilation adds minor value.\"\n",
    "}\n",
    "\n",
    "friendly_names = [feature_rename.get(f, f) for f in features]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Horizontal bar plot\n",
    "bars = ax.barh(models, shap_values, height=bar_width, color=color)\n",
    "ax.set_yticks(models)\n",
    "ax.set_yticklabels(friendly_names)\n",
    "ax.invert_yaxis()  # highest SHAP on top\n",
    "ax.set_xlabel(\"Mean Absolute SHAP Contribution (€)\")\n",
    "ax.set_title(\"Top 10 Features by SHAP Contribution in Euros\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Minimum offset to avoid overlap with values\n",
    "min_offset = max(shap_values) * 0.2\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    val = bar.get_width()\n",
    "    # Value label\n",
    "    ax.text(val + min_offset*0.2, i, f\"€{val:,.0f}\", va='center', fontsize=10)\n",
    "    \n",
    "    # Note, wrapped and placed to the right of bar\n",
    "    note = notes_dict.get(features[i], \"\")\n",
    "    if note:\n",
    "        wrapped_note = fill(note, width=40)  # wrap at ~40 chars\n",
    "        note_x = val + min_offset\n",
    "        ax.text(note_x, i, wrapped_note, va='center', ha='left', fontsize=9,\n",
    "                color='black', fontstyle='italic',\n",
    "                bbox=dict(facecolor='lightgray', alpha=0.3, boxstyle='round,pad=0.2'))\n",
    "\n",
    "# Extend x-limits to fit notes\n",
    "ax.set_xlim(0, max(shap_values) + 3*min_offset)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a624a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from src.features.feature_engineering.feature_engineering import add_luxury_features, add_luxury_interactions, LUXURY_AMENITIES, LUXURY_AMENITIES_WEIGHTS\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# ------------------- Compute luxury scores -------------------\n",
    "df_lux = add_luxury_features(df_clean)\n",
    "df_lux = add_luxury_interactions(df_lux)\n",
    "\n",
    "# ------------------- Top Luxury Features -------------------\n",
    "luxury_weights = {k: v for k, v in LUXURY_AMENITIES_WEIGHTS.items() if k in df_lux.columns}\n",
    "avg_contribution = {feature: (df_lux[feature] * weight).mean() for feature, weight in luxury_weights.items()}\n",
    "\n",
    "df_bar = pd.DataFrame({\n",
    "    \"Feature\": list(avg_contribution.keys()),\n",
    "    \"Value\": list(avg_contribution.values())\n",
    "}).sort_values(\"Value\", ascending=True)\n",
    "\n",
    "feature_rename = {\n",
    "    \"has_lift\": \"Lift\",\n",
    "    \"has_sauna\": \"Sauna\",\n",
    "    \"has_domotica\": \"Smart Home Features\",\n",
    "    \"has_airconditioning\": \"Air Conditioning\",\n",
    "    \"has_zwembad\": \"Pool\"\n",
    "}\n",
    "\n",
    "df_bar[\"FeatureFriendly\"] = df_bar[\"Feature\"].map(feature_rename)\n",
    "\n",
    "# ------------------- Luxury Interaction Features -------------------\n",
    "interaction_features = [\"luxury_x_price_m2\", \"luxury_x_size\", \"luxury_x_inhabitants\"]\n",
    "importances = [0.35, 0.25, 0.15]  # replace with actual model importances\n",
    "\n",
    "df_interaction = pd.DataFrame({\n",
    "    \"Feature\": interaction_features,\n",
    "    \"Value\": importances\n",
    "}).sort_values(\"Value\", ascending=True)\n",
    "\n",
    "feature_rename_interactions = {\n",
    "    \"luxury_x_price_m2\": \"Luxury × Neighborhood Price per m²\",\n",
    "    \"luxury_x_size\": \"Luxury × Size\",\n",
    "    \"luxury_x_inhabitants\": \"Luxury × Population Density\"\n",
    "}\n",
    "\n",
    "df_interaction[\"FeatureFriendly\"] = df_interaction[\"Feature\"].map(feature_rename_interactions)\n",
    "\n",
    "# ------------------- Plot side by side -------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=False)\n",
    "bar_width = 0.6\n",
    "\n",
    "# Palettes\n",
    "palette_luxury = sns.light_palette(\"skyblue\", n_colors=len(df_bar))\n",
    "palette_interaction = sns.light_palette(\"orange\", n_colors=len(df_interaction))\n",
    "\n",
    "# --- Left: Top Luxury Features ---\n",
    "bars1 = axes[0].barh(df_bar[\"FeatureFriendly\"], df_bar[\"Value\"], color=palette_luxury, height=bar_width)\n",
    "axes[0].set_xlabel(\"Average Contribution\")\n",
    "axes[0].set_title(\"Top Luxury Features\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    val = bar.get_width()\n",
    "    axes[0].text(val + val*0.02, i, f\"{val:.2f}\", va='center', fontsize=10)\n",
    "\n",
    "# Left subplot: Top Luxury Features\n",
    "axes[0].text(0.95, 0.05, \n",
    "             \"Shows which individual luxury amenities contribute most to the\\nluxury score, averaged across listings.\",\n",
    "             transform=axes[0].transAxes, fontsize=10, fontstyle='italic', color='black',\n",
    "             ha='right', va='bottom',\n",
    "             bbox=dict(facecolor='lightgray', alpha=0.3, boxstyle='round,pad=0.5'))\n",
    "\n",
    "# --- Right: Luxury Interaction Features ---\n",
    "bars2 = axes[1].barh(df_interaction[\"FeatureFriendly\"], df_interaction[\"Value\"], color=palette_interaction, height=bar_width)\n",
    "axes[1].set_xlabel(\"Relative Importance\")\n",
    "axes[1].set_title(\"Luxury Interaction Features\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    val = bar.get_width()\n",
    "    axes[1].text(val + val*0.02, i, f\"{val:.2f}\", va='center', fontsize=10)\n",
    "\n",
    "# Right subplot: Luxury Interaction Features\n",
    "axes[1].text(0.95, 0.05, \n",
    "             \"Shows how luxury features impact price in context:\\nneighborhood price per m², home size, and neighborhood population density.\",\n",
    "             transform=axes[1].transAxes, fontsize=10, fontstyle='italic', color='black',\n",
    "             ha='right', va='bottom',\n",
    "             bbox=dict(facecolor='lightgray', alpha=0.3, boxstyle='round,pad=0.5'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Sort trials by number\n",
    "trials = sorted(study_xgb.trials, key=lambda t: t.number)\n",
    "trial_numbers = [t.number for t in trials]\n",
    "objective_values = [t.value for t in trials]\n",
    "\n",
    "# Compute best-so-far values\n",
    "best_so_far = []\n",
    "current_best = float('inf')  # minimizing objective\n",
    "best_trial_index = 0\n",
    "for i, value in enumerate(objective_values):\n",
    "    if value < current_best:\n",
    "        current_best = value\n",
    "        best_trial_index = i\n",
    "    best_so_far.append(current_best)\n",
    "\n",
    "# Best trial info\n",
    "best_trial = trials[best_trial_index]\n",
    "best_params = best_trial.params\n",
    "best_value = best_trial.value\n",
    "\n",
    "# Round numeric parameters to 3 decimals\n",
    "param_text = \"\\n\".join([f\"{k}: {round(v,3) if isinstance(v,(int,float)) else v}\" \n",
    "                        for k, v in best_params.items()])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Scatter trial values\n",
    "ax.scatter(trial_numbers, objective_values, color='skyblue', label='Trial Values', edgecolor='k')\n",
    "\n",
    "# Best-so-far line\n",
    "ax.plot(trial_numbers, best_so_far, color='red', linewidth=2, label='Best-So-Far')\n",
    "\n",
    "# Highlight best trial\n",
    "ax.scatter(best_trial.number, best_value, color='green', s=150, marker='*', label='Best Trial', edgecolor='k')\n",
    "\n",
    "# Annotate best trial with a box\n",
    "ax.annotate(f'Best Trial #{best_trial.number}\\nValue: {best_value:.4f}\\n{param_text}',\n",
    "            xy=(best_trial.number, best_value),\n",
    "            xytext=(best_trial.number + 0.5, best_value + 0.5),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightyellow\", alpha=0.4),\n",
    "            fontsize=10)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Objective Value')\n",
    "ax.set_title('Optuna XGBoost Optimization Convergence')\n",
    "\n",
    "# Legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Compute residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Define extreme outliers: top 5% of absolute residuals\n",
    "threshold = np.percentile(np.abs(residuals), 95)\n",
    "outliers_mask = np.abs(residuals) >= threshold\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot normal listings\n",
    "plt.scatter(\n",
    "    y_test[~outliers_mask],\n",
    "    y_test_pred[~outliers_mask],\n",
    "    alpha=0.5,\n",
    "    color='skyblue',\n",
    "    label=\"Normal listings\",\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Plot extreme residuals\n",
    "plt.scatter(\n",
    "    y_test[outliers_mask],\n",
    "    y_test_pred[outliers_mask],\n",
    "    color=\"red\",\n",
    "    label=\"Extreme listings\",\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Diagonal line (perfect prediction)\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "plt.plot([0, max_val], [0, max_val], color=\"black\", linestyle=\"--\", label=\"Perfect prediction\")\n",
    "\n",
    "# Annotate a few largest outliers by absolute residual\n",
    "num_annotations = 5\n",
    "outliers_df = pd.DataFrame({\n",
    "    'y_true': y_test[outliers_mask],\n",
    "    'y_pred': y_test_pred[outliers_mask],\n",
    "    'residual': residuals[outliers_mask],\n",
    "    'size': X_test.loc[outliers_mask, 'size_num']  # replace 'size' with your feature name\n",
    "})\n",
    "outliers_df['abs_residual'] = np.abs(outliers_df['residual'])\n",
    "top_outliers = outliers_df.nlargest(num_annotations, 'abs_residual').copy()\n",
    "\n",
    "# Initialize annotation positions slightly offset\n",
    "top_outliers['x_offset'] = 10\n",
    "top_outliers['y_offset'] = 10\n",
    "\n",
    "# Simple repel for overlapping annotations\n",
    "min_distance = 15  # in points\n",
    "for i in range(len(top_outliers)):\n",
    "    for j in range(i):\n",
    "        dx = top_outliers.iloc[i]['x_offset'] - top_outliers.iloc[j]['x_offset']\n",
    "        dy = top_outliers.iloc[i]['y_offset'] - top_outliers.iloc[j]['y_offset']\n",
    "        distance = np.hypot(dx, dy)\n",
    "        if distance < min_distance:\n",
    "            top_outliers.at[top_outliers.index[i], 'y_offset'] += min_distance - distance\n",
    "\n",
    "# Add annotations with repelling offsets\n",
    "for idx, row in top_outliers.iterrows():\n",
    "    plt.annotate(\n",
    "        f\"Size: {row['size']}\\nPrice: €{row['y_true']:,.0f}\",\n",
    "        xy=(row['y_true'], row['y_pred']),\n",
    "        xytext=(row['x_offset'], row['y_offset']),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightyellow\", alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1)\n",
    "    )\n",
    "\n",
    "# Format axes in euros with thousands separator\n",
    "formatter = FuncFormatter(lambda x, _: f\"€{int(x):,}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Labels, title, legend\n",
    "plt.xlabel(\"Actual Price (€)\")\n",
    "plt.ylabel(\"Predicted Price (€)\")\n",
    "plt.title(\"Predicted vs Actual Prices with Extreme Listings Highlighted\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Define results for MAPE only ---\n",
    "results_mape = {\n",
    "    \"Last Week Model\": {\n",
    "        'Test MAPE (%)': 10.0  # example value\n",
    "    },\n",
    "    \"This Week Model\": {\n",
    "        'Test MAPE (%)': 9.05\n",
    "    }\n",
    "}\n",
    "\n",
    "df_mape = pd.DataFrame(results_mape)\n",
    "\n",
    "# Bar parameters\n",
    "bar_width = 0.35\n",
    "colors = ['skyblue', 'orange']\n",
    "\n",
    "metrics = df_mape.index\n",
    "y_pos = np.arange(len(metrics))\n",
    "\n",
    "plt.figure(figsize=(8, 2.5))\n",
    "\n",
    "# Horizontal bars: two bars per metric\n",
    "plt.barh(y_pos - bar_width/2, df_mape['Last Week Model'], height=bar_width, color=colors[0], label='Last Week Model')\n",
    "plt.barh(y_pos + bar_width/2, df_mape['This Week Model'], height=bar_width, color=colors[1], label='This Week Model')\n",
    "\n",
    "plt.yticks(y_pos, metrics)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Data labels\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, col in enumerate(df_mape.columns):\n",
    "        val = df_mape.loc[metric, col]\n",
    "        offset = -bar_width/2 if j == 0 else bar_width/2\n",
    "        plt.text(val + 0.1, i + offset, f\"{val:.2f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"MAPE (%)\")\n",
    "plt.title(\"XGBoost Test MAPE: Last Week vs This Week Model\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fce80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Define results for RMSE and MAE ---\n",
    "results = {\n",
    "    \"Last Week Model\": {\n",
    "        'Test RMSE (€)': 211172,\n",
    "        'Test MAE (€)': 72964\n",
    "    },\n",
    "    \"This Week Model\": {\n",
    "        'Test RMSE (€)': 205449,\n",
    "        'Test MAE (€)': 75442\n",
    "    }\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Bar parameters\n",
    "bar_width = 0.35\n",
    "colors = ['skyblue', 'orange']\n",
    "\n",
    "metrics = df_results.index\n",
    "y_pos = np.arange(len(metrics))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Horizontal bars: two bars per metric\n",
    "plt.barh(y_pos - bar_width/2, df_results['Last Week Model'], height=bar_width, color=colors[0], label='Phase 3A Model')\n",
    "plt.barh(y_pos + bar_width/2, df_results['This Week Model'], height=bar_width, color=colors[1], label='Current Phase Model')\n",
    "\n",
    "plt.yticks(y_pos, metrics)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Data labels\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, col in enumerate(df_results.columns):\n",
    "        val = df_results.loc[metric, col]\n",
    "        offset = -bar_width/2 if j == 0 else bar_width/2\n",
    "        plt.text(val + 5000, i + offset, f\"{val:,.0f}\", va='center', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Error (€)\")\n",
    "plt.title(\"XGBoost Test RMSE & MAE: Phase 3A vs Phase 3B Model Error Metrics\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Define metrics ---\n",
    "results = {\n",
    "    \"Last Week Model\": {\n",
    "        'Train RMSE (€)': 87196,\n",
    "        'Val RMSE (€)': 182978,\n",
    "        'Test RMSE (€)': 211172,\n",
    "        'Train MAE (€)': 45665,\n",
    "        'Val MAE (€)': 81436,\n",
    "        'Test MAE (€)': 72964\n",
    "    },\n",
    "    \"This Week Model\": {\n",
    "        'Train RMSE (€)': 112722,\n",
    "        'Val RMSE (€)': 152651,\n",
    "        'Test RMSE (€)': 205449,\n",
    "        'Train MAE (€)': 54341,\n",
    "        'Val MAE (€)': 75441,\n",
    "        'Test MAE (€)': 75442\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "metrics = df.index\n",
    "y_pos = np.arange(len(metrics))\n",
    "bar_width = 0.25\n",
    "colors = ['skyblue', 'orange']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Horizontal bars: last week vs this week\n",
    "plt.barh(y_pos - bar_width/2, df['Last Week Model'], height=bar_width, color=colors[0], label='Last Week Model')\n",
    "plt.barh(y_pos + bar_width/2, df['This Week Model'], height=bar_width, color=colors[1], label='This Week Model')\n",
    "\n",
    "plt.yticks(y_pos, metrics)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Data labels\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, col in enumerate(df.columns):\n",
    "        val = df.loc[metric, col]\n",
    "        offset = -bar_width/2 if j == 0 else bar_width/2\n",
    "        plt.text(val + 5000, i + offset, f\"{val:,.0f}\", va='center', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Error (€)\")\n",
    "plt.title(\"XGBoost Train / Val / Test RMSE & MAE: Last Week vs This Week Model\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Define MAPE metrics ---\n",
    "results_mape = {\n",
    "    \"Last Week Model\": {\n",
    "        'Train MAPE (%)': 6.5,\n",
    "        'Val MAPE (%)': 10.52,\n",
    "        'Test MAPE (%)': 8.5\n",
    "    },\n",
    "    \"This Week Model\": {\n",
    "        'Train MAPE (%)': 7.57,\n",
    "        'Val MAPE (%)': 10.34,\n",
    "        'Test MAPE (%)': 9.05\n",
    "    }\n",
    "}\n",
    "\n",
    "df_mape = pd.DataFrame(results_mape)\n",
    "\n",
    "metrics = df_mape.index\n",
    "y_pos = np.arange(len(metrics))\n",
    "bar_width = 0.25\n",
    "colors = ['skyblue', 'orange']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Horizontal bars: last week vs this week\n",
    "plt.barh(y_pos - bar_width/2, df_mape['Last Week Model'], height=bar_width, color=colors[0], label='Last Week Model')\n",
    "plt.barh(y_pos + bar_width/2, df_mape['This Week Model'], height=bar_width, color=colors[1], label='This Week Model')\n",
    "\n",
    "plt.yticks(y_pos, metrics)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Data labels\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, col in enumerate(df_mape.columns):\n",
    "        val = df_mape.loc[metric, col]\n",
    "        offset = -bar_width/2 if j == 0 else bar_width/2\n",
    "        plt.text(val + 0.1, i + offset, f\"{val:.2f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"MAPE (%)\")\n",
    "plt.title(\"XGBoost Train / Val / Test MAPE: Last Week vs This Week Model\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Results ---\n",
    "results_rmse = {\n",
    "    \"Last Week Model\": {\n",
    "        \"Train RMSE (€)\": 87196,\n",
    "        \"Validation RMSE (€)\": 182977.84,\n",
    "        \"Test RMSE (€)\": 211172\n",
    "    },\n",
    "    \"This Week Model\": {\n",
    "        \"Train RMSE (€)\": 112722.15,\n",
    "        \"Validation RMSE (€)\": 152651.16,\n",
    "        \"Test RMSE (€)\": 205448.59\n",
    "    }\n",
    "}\n",
    "\n",
    "df_rmse = pd.DataFrame(results_rmse)\n",
    "bar_width = 0.35\n",
    "colors = [\"skyblue\", \"orange\"]\n",
    "\n",
    "metrics = df_rmse.index\n",
    "y_pos = np.arange(len(metrics))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(y_pos - bar_width/2, df_rmse[\"Last Week Model\"], height=bar_width, color=colors[0],\n",
    "         label=\"Last Phase(3A): XGB + Basic FE +\\nOptuna\")\n",
    "plt.barh(y_pos + bar_width/2, df_rmse[\"This Week Model\"], height=bar_width, color=colors[1],\n",
    "         label=\"This Phase (3B): XGB + Log(target) +\\nExtended FE + Optuna\")\n",
    "\n",
    "plt.yticks(y_pos, metrics)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# --- Format x-axis with thousands separator and \"k\" suffix ---\n",
    "def k_formatter(x, pos):\n",
    "    if x >= 1000:\n",
    "        return f\"{x/1000:.0f}k\"\n",
    "    return f\"{x:.0f}\"\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
    "\n",
    "# --- Add grid for clarity ---\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Data labels ---\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, col in enumerate(df_rmse.columns):\n",
    "        val = df_rmse.loc[metric, col]\n",
    "        offset = -8000 if j == 0 else 8000\n",
    "        plt.text(val + offset, i - bar_width/2 + j*bar_width,\n",
    "                 f\"{val:,.0f}\", va='center', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Error (€)\")\n",
    "plt.title(\"RMSE Comparison: Phase 3A vs 3B Model (Train / Validation / Test)\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Results ---\n",
    "results = {\n",
    "    \"Metric\": [\"MAE (€)\", \"MAPE (%)\"],\n",
    "    \"Last Week\": [72964, 8.5],\n",
    "    \"This Week\": [75441.86, 9.05]\n",
    "}\n",
    "df = pd.DataFrame(results).set_index(\"Metric\")\n",
    "\n",
    "# --- Bar setup ---\n",
    "bar_width = 0.35\n",
    "colors = [\"skyblue\", \"orange\"]\n",
    "y_pos = np.arange(1)  # one bar per subplot\n",
    "\n",
    "# --- Formatter for thousands ---\n",
    "thousands_formatter = FuncFormatter(lambda x, pos: f'{int(x/1000)}k')\n",
    "\n",
    "# --- Create subplots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# -------------------- MAE (€) --------------------\n",
    "ax1 = axes[0]\n",
    "ax1.barh(y_pos - bar_width/2, df.loc[\"MAE (€)\", \"Last Week\"], height=bar_width, color=colors[0], label=\"Phase 3A\")\n",
    "ax1.barh(y_pos + bar_width/2, df.loc[\"MAE (€)\", \"This Week\"], height=bar_width, color=colors[1], label=\"Current (3B)\")\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels([\"MAE (€)\"])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel(\"Error (€)\")\n",
    "ax1.set_title(\"Test MAE Comparison\")\n",
    "ax1.xaxis.set_major_formatter(thousands_formatter)\n",
    "\n",
    "# Add value labels\n",
    "for j, col in enumerate([\"Last Week\", \"This Week\"]):\n",
    "    val = df.loc[\"MAE (€)\", col]\n",
    "    offset = -2000 if j == 0 else 2000\n",
    "    ax1.text(val + offset, y_pos[0] - bar_width/2 + j*bar_width, f\"{val:,.0f}€\", va='center', fontsize=10)\n",
    "\n",
    "# -------------------- MAPE (%) --------------------\n",
    "ax2 = axes[1]\n",
    "ax2.barh(y_pos - bar_width/2, df.loc[\"MAPE (%)\", \"Last Week\"], height=bar_width, color=colors[0])\n",
    "ax2.barh(y_pos + bar_width/2, df.loc[\"MAPE (%)\", \"This Week\"], height=bar_width, color=colors[1])\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels([\"MAPE (%)\"])\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel(\"MAPE (%)\")\n",
    "ax2.set_title(\"Test MAPE Comparison\")\n",
    "ax2.set_xlim(0, max(df.loc[\"MAPE (%)\"]) * 1.5)\n",
    "\n",
    "# Add value labels\n",
    "for j, col in enumerate([\"Last Week\", \"This Week\"]):\n",
    "    val = df.loc[\"MAPE (%)\", col]\n",
    "    offset = -0.3 if j == 0 else 0.3\n",
    "    ax2.text(val + offset, y_pos[0] - bar_width/2 + j*bar_width, f\"{val:.2f}%\", va='center', fontsize=10)\n",
    "\n",
    "# -------------------- Layout --------------------\n",
    "ax1.legend(loc='lower right', frameon=True)\n",
    "fig.suptitle(\"Model Comparison: Phase 3A vs Current Model Test Set Performance (MAE & MAPE)\", fontsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf73fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
