{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc88793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "tracking_uri = \"../logs/mlruns\"\n",
    "os.makedirs(os.path.join(tracking_uri, \".trash\"), exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"house_price_prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Adjust the path to your project root folder\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(\"..\")\n",
    ")  # from notebooks/ up one level\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.data_loading.data_loading.data_loader import load_data_from_json\n",
    "from src.data_loading.preprocessing.preprocessing import preprocess_df\n",
    "from src.data_loading.preprocessing.imputation import impute_missing_values\n",
    "\n",
    "\n",
    "# go two levels up from notebook dir -> project root\n",
    "ROOT = (\n",
    "    Path(__file__).resolve().parents[2]\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parents[1]\n",
    ")\n",
    "CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"preprocessing_config.yaml\"\n",
    ")\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "df_raw = load_data_from_json(\"../data/parsed_json/*.json\")\n",
    "df_clean = preprocess_df(\n",
    "    df_raw,\n",
    "    drop_raw=CONFIG[\"preprocessing\"][\"drop_raw\"],\n",
    "    numeric_cols=CONFIG[\"preprocessing\"][\"numeric_cols\"],\n",
    ")\n",
    "df_clean = impute_missing_values(\n",
    "    df_clean, CONFIG[\"preprocessing\"][\"imputation\"]\n",
    ")\n",
    "# Drop price_num NaNs for the training of the model\n",
    "df_clean = df_clean[df_clean[\"price_num\"].notna()]\n",
    "df_clean.drop(columns=[\"living_area\"], inplace=True)\n",
    "\n",
    "\n",
    "# df_clean = df_clean[:100] \n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff54eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Adjust ROOT to your project root ---\n",
    "ROOT = Path(\"C:/Users/LaurynasBaltrusaitis/OneDrive - Adaptfy BV/Desktop/Education/git_personal_repos\")\n",
    "\n",
    "CONFIG_PATH = ROOT / \"house_price_prediction_project\" / \"config\" / \"model_config.yaml\"\n",
    "\n",
    "from src.features.data_prep_for_modelling.data_preparation import load_geo_config\n",
    "from src.features.feature_engineering.location_feature_enrichment import load_cache\n",
    "\n",
    "# 1️⃣ Test load_geo_config\n",
    "geo_cache_file, amenities_df, amenity_radius_map = load_geo_config(CONFIG_PATH)\n",
    "print(\"Geo cache file:\", geo_cache_file)\n",
    "print(\"Amenities df:\", amenities_df.shape if amenities_df is not None else None)\n",
    "print(\"Amenity radius map:\", amenity_radius_map)\n",
    "\n",
    "# 2️⃣ Test load_cache\n",
    "lat_lon_cache = load_cache(geo_cache_file)\n",
    "print(\"Number of addresses in cache:\", len(lat_lon_cache))\n",
    "print(\"Sample from cache:\", list(lat_lon_cache.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cache_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ad5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.data_prep_for_modelling.data_preparation import prepare_data\n",
    "\n",
    "FEATURES_CONFIG_PATH = (\n",
    "    ROOT / \"house_price_prediction_project\" / \"config\" / \"model_config.yaml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1263882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.evaluate import ModelEvaluator\n",
    "from src.model.mlflow_logger import MLFlowLogger\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "logger = MLFlowLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../data/df_with_lat_lon_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b73fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "from src.features.data_prep_for_modelling.data_preparation import prepare_data_from_config\n",
    "\n",
    "\n",
    "config_path = FEATURES_AND_MODEL_CONFIG_PATH  # e.g., ROOT / \"config/model_config.yaml\"\n",
    "from src.features.feature_engineering.location_feature_enrichment import load_cache\n",
    "\n",
    "\n",
    "# Call the wrapper\n",
    "X_train, X_test, y_train, y_test, X_val, y_val, scaler, meta = prepare_data_from_config(\n",
    "    df=df_clean,\n",
    "    config_path=config_path,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng_geoloc_exp\"\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape if X_val is not None else None)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.copy()\n",
    "X_test_final = X_test.copy()\n",
    "X_val_final = X_val.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train_final.shape)\n",
    "print(\"validation shape:\", X_val_final.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with log-transform\n",
    "\n",
    "from functools import partial\n",
    "from src.model.objectives_optuna import unified_objective\n",
    "\n",
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "study_xgb = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "objective_xgb_partial = partial(\n",
    "    unified_objective,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng_geoloc_exp\",\n",
    "    df=df_clean,\n",
    "    features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    use_log=True,  \n",
    "    n_splits=5,\n",
    "    use_extended_features=True,\n",
    "    use_geo_amenities=True,\n",
    ")\n",
    "study_xgb.optimize(objective_xgb_partial, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"postal_code_clean\", \"lat\", \"lon\"]  # add any other object columns if needed\n",
    "\n",
    "X_train_final = X_train_final.drop(columns=[c for c in cols_to_drop if c in X_train_final.columns])\n",
    "X_val_final   = X_val_final.drop(columns=[c for c in cols_to_drop if c in X_val_final.columns])\n",
    "X_test_final  = X_test_final.drop(columns=[c for c in cols_to_drop if c in X_test_final.columns])\n",
    "\n",
    "# --- XGBoost ---\n",
    "evaluator = ModelEvaluator(target_transform=np.log1p, inverse_transform=np.expm1)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "\n",
    "trained_xgb, y_train_pred, y_val_pred, y_test_pred, results_xgb = evaluator.evaluate(\n",
    "    model=None,  # not used in XGBoost.train\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_final,\n",
    "    y_test=y_test,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    use_xgb_train=True,\n",
    "    model_params=best_xgb_params,  # <--- crucial\n",
    "    fit_params={\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    ")\n",
    "logger.log_model(trained_xgb, \"XGB_Optuna_LogTransformed_feature_eng\", results_xgb, use_xgb_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa53ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from src.model.cv_helpers import prepare_base_data, prepare_fold_features\n",
    "\n",
    "# Number of folds (should match your CV setup)\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Prepare base data (features + target)\n",
    "X_full, y_full = prepare_base_data(df_clean, FEATURES_AND_MODEL_CONFIG_PATH, \"xgboost_early_stopping_optuna_feature_eng_geoloc_exp\")\n",
    "\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "fold_importances = {imp_type: [] for imp_type in importance_types}\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "    X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "    y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "\n",
    "    # Prepare fold-wise features (ensure extended feature engineering matches training)\n",
    "    X_train_fold, X_val_fold, _, _ = prepare_fold_features(X_train, X_val, features_config=FEATURES_AND_MODEL_CONFIG_PATH, use_extended_features=True, enable_cache_save=False)\n",
    "\n",
    "    cols_to_drop = [\"size_num\", \"lat\", \"lon\"]  # or just one of them\n",
    "    X_train_fold = X_train_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    X_val_fold = X_val_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Train XGBoost on this fold using best params\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=np.log1p(y_train))\n",
    "    dval = xgb.DMatrix(X_val_fold, label=np.log1p(y_val))\n",
    "    \n",
    "    model_fold = xgb.train(\n",
    "        params=best_xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Collect importance for each type\n",
    "    for imp_type in importance_types:\n",
    "        imp_dict = model_fold.get_score(importance_type=imp_type)\n",
    "        df_imp = pd.DataFrame.from_dict(imp_dict, orient=\"index\", columns=[imp_type])\n",
    "        df_imp.index.name = \"feature\"\n",
    "        fold_importances[imp_type].append(df_imp)\n",
    "\n",
    "# --- Aggregate across folds ---\n",
    "agg_importances = {}\n",
    "for imp_type, dfs in fold_importances.items():\n",
    "    # Combine all folds into a single dataframe\n",
    "    df_all = pd.concat(dfs, axis=1).fillna(0)\n",
    "    df_all[\"mean\"] = df_all.mean(axis=1)\n",
    "    df_all = df_all.sort_values(by=\"mean\", ascending=False)\n",
    "    agg_importances[imp_type] = df_all\n",
    "    print(f\"\\nTop 10 features by mean {imp_type} across folds:\")\n",
    "    print(df_all[\"mean\"].head(100))\n",
    "    \n",
    "    # Plot top 20\n",
    "    df_all[\"mean\"].head(200).plot.barh(figsize=(10,6), title=f\"Top 20 features by mean {imp_type} across folds\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "fold_shap_values = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "    X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "    y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "    \n",
    "    # Prepare fold-wise features\n",
    "    X_train_fold, X_val_fold, _, _ = prepare_fold_features(X_train, X_val, features_config=FEATURES_AND_MODEL_CONFIG_PATH, use_extended_features=True, enable_cache_save=False)\n",
    "    cols_to_drop = [\"size_num\", \"lat\", \"lon\"] \n",
    "    X_train_fold = X_train_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    X_val_fold = X_val_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Train XGBoost Booster\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=np.log1p(y_train))\n",
    "    dval = xgb.DMatrix(X_val_fold, label=np.log1p(y_val))\n",
    "    \n",
    "    model_fold = xgb.train(\n",
    "        params=best_xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # SHAP for Booster\n",
    "    explainer = shap.TreeExplainer(model_fold)\n",
    "    shap_values = explainer.shap_values(dval)  # DMatrix\n",
    "    \n",
    "    # Store mean absolute SHAP values per feature for this fold\n",
    "    fold_shap_values.append(pd.DataFrame({\n",
    "        \"feature\": X_val_fold.columns,\n",
    "        \"mean_abs_shap\": np.abs(shap_values).mean(axis=0)\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Combine all folds\n",
    "df_shap_all = pd.concat(fold_shap_values)\n",
    "\n",
    "# Group by feature and compute mean across folds\n",
    "agg_shap = df_shap_all.groupby(\"feature\")[\"mean_abs_shap\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by mean absolute SHAP value across folds:\")\n",
    "print(agg_shap.head(10))\n",
    "\n",
    "# Prepare data\n",
    "top_features = agg_shap.head(10).sort_values()\n",
    "features = top_features.index\n",
    "shap_values = top_features.values\n",
    "models = np.arange(len(features))\n",
    "bar_width = 0.6\n",
    "color = 'skyblue'\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Horizontal bar plot\n",
    "ax.barh(models, shap_values, height=bar_width, color=color)\n",
    "\n",
    "# Y-axis labels\n",
    "ax.set_yticks(models)\n",
    "ax.set_yticklabels(features)\n",
    "ax.invert_yaxis()  # highest SHAP on top\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Mean Absolute SHAP Value\")\n",
    "ax.set_title(\"Top 10 Features by SHAP Value\")\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(shap_values):\n",
    "    ax.text(val + 0.01 * max(shap_values), i, f\"{val:.3f}\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all folds\n",
    "df_shap_all = pd.concat(fold_shap_values)\n",
    "\n",
    "# Group by feature and compute mean across folds\n",
    "agg_shap = df_shap_all.groupby(\"feature\")[\"mean_abs_shap\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features by mean absolute SHAP value across folds:\")\n",
    "print(agg_shap.head(10))\n",
    "\n",
    "# Plot\n",
    "agg_shap.head(10).sort_values().plot.barh(figsize=(10,6), title=\"Top 10 features by SHAP value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "def shap_to_euros(model, X, target_transform_inverse=np.expm1):\n",
    "    \"\"\"\n",
    "    Compute SHAP values in € for a trained XGBoost model with log1p target.\n",
    "    \"\"\"\n",
    "    # Predict log1p(price)\n",
    "    dmatrix = xgb.DMatrix(X)\n",
    "    pred_log = model.predict(dmatrix)\n",
    "\n",
    "    # Get full SHAP values aligned with all columns\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_array = explainer.shap_values(X)  # (n_samples, n_features)\n",
    "\n",
    "    # Sanity check\n",
    "    if shap_values_array.shape[1] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Mismatch: SHAP shape {shap_values_array.shape[1]} vs X {X.shape[1]}\"\n",
    "        )\n",
    "\n",
    "    # Convert SHAP deltas to € scale\n",
    "    price_contrib = np.expm1(pred_log[:, None] + shap_values_array) - np.expm1(pred_log[:, None])\n",
    "    df_shap_euros = pd.DataFrame(price_contrib, columns=X.columns)\n",
    "\n",
    "    mean_abs_euros = df_shap_euros.abs().mean().sort_values(ascending=False)\n",
    "    return mean_abs_euros\n",
    "\n",
    "# Usage:\n",
    "mean_abs_shap_euros = shap_to_euros(model_fold, X_val_fold)\n",
    "print(mean_abs_shap_euros.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b321d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Updated top 10 SHAP-Euro contributions\n",
    "top_features = pd.Series({\n",
    "    \"log_size_num\": 349033.09,\n",
    "    \"price_per_m2_neighborhood\": 77572.78,\n",
    "    \"outdoor_area_ratio\": 19810.93,\n",
    "    \"luxury_x_price_m2\": 12859.01,\n",
    "    \"size_per_luxury\": 12562.23,\n",
    "    \"dist_to_center_bin_encoded\": 12214.45,\n",
    "    \"energy_label_encoded\": 11375.25,\n",
    "    \"ownership_type_Other\": 10560.33,\n",
    "    \"luxury_x_size\": 8401.32,\n",
    "    \"has_mechanische_ventilatie\": 7442.82\n",
    "}).sort_values()\n",
    "\n",
    "features = top_features.index\n",
    "shap_values = top_features.values\n",
    "models = np.arange(len(features))\n",
    "bar_width = 0.6\n",
    "color = 'skyblue'\n",
    "\n",
    "# Friendly feature names\n",
    "feature_rename = {\n",
    "    \"log_size_num\": \"Property Size (log m²)\",\n",
    "    \"price_per_m2_neighborhood\": \"Price per m² by Neighborhood\",\n",
    "    \"outdoor_area_ratio\": \"Outdoor Area Ratio\",\n",
    "    \"luxury_x_price_m2\": \"Luxury × Price per m²\",\n",
    "    \"size_per_luxury\": \"Size per Luxury Feature\",\n",
    "    \"dist_to_center_bin_encoded\": \"Distance to Center (binned)\",\n",
    "    \"energy_label_encoded\": \"Energy Label\",\n",
    "    \"ownership_type_Other\": \"Other Ownership Type\",\n",
    "    \"luxury_x_size\": \"Luxury × Size\",\n",
    "    \"has_mechanische_ventilatie\": \"Mechanical Ventilation\"\n",
    "}\n",
    "\n",
    "# Notes dictionary\n",
    "notes_dict = {\n",
    "    \"log_size_num\": \"Large properties dominate pricing.\",\n",
    "    \"price_per_m2_neighborhood\": \"Captures local market variation.\",\n",
    "    \"outdoor_area_ratio\": \"More outdoor space slightly increases value.\",\n",
    "    \"luxury_x_price_m2\": \"Luxury features amplify per-m² price.\",\n",
    "    \"size_per_luxury\": \"More space per luxury feature adds value.\",\n",
    "    \"dist_to_center_bin_encoded\": \"Closer to center generally increases price.\",\n",
    "    \"energy_label_encoded\": \"Better energy label slightly increases value.\",\n",
    "    \"ownership_type_Other\": \"Non-standard ownership has moderate effect.\",\n",
    "    \"luxury_x_size\": \"Luxury features in bigger homes matter more.\",\n",
    "    \"has_mechanische_ventilatie\": \"Mechanical ventilation slightly increases value.\"\n",
    "}\n",
    "\n",
    "# Map friendly names\n",
    "friendly_names = [feature_rename.get(f, f) for f in features]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Horizontal bar plot\n",
    "bars = ax.barh(models, shap_values, height=bar_width, color=color)\n",
    "\n",
    "# Y-axis labels (friendly names)\n",
    "ax.set_yticks(models)\n",
    "ax.set_yticklabels(friendly_names)\n",
    "ax.invert_yaxis()  # highest SHAP on top\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Mean Absolute SHAP Contribution (€)\")\n",
    "ax.set_title(\"Top 10 Features by SHAP Contribution in Euros\")\n",
    "\n",
    "# Determine far-right position for notes\n",
    "note_x = max(shap_values) * 1.15\n",
    "\n",
    "# Add value labels and notes\n",
    "for i, bar in enumerate(bars):\n",
    "    val = bar.get_width()\n",
    "    ax.text(val + max(shap_values)*0.01, i, f\"€{val:,.0f}\", va='center', fontsize=10)\n",
    "    \n",
    "    note = notes_dict.get(features[i], \"\")\n",
    "    if note:\n",
    "        ax.text(note_x, i, note, va='center', ha='left', fontsize=9,\n",
    "                color='black', fontstyle='italic',\n",
    "                bbox=dict(facecolor='lightgray', alpha=0.3, boxstyle='round,pad=0.3'))\n",
    "\n",
    "# Extend x-limits to fit notes\n",
    "ax.set_xlim(0, note_x * 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Sort trials by number\n",
    "trials = sorted(study_xgb.trials, key=lambda t: t.number)\n",
    "trial_numbers = [t.number for t in trials]\n",
    "objective_values = [t.value for t in trials]\n",
    "\n",
    "# Compute best-so-far values\n",
    "best_so_far = []\n",
    "current_best = float('inf')  # minimizing objective\n",
    "best_trial_index = 0\n",
    "for i, value in enumerate(objective_values):\n",
    "    if value < current_best:\n",
    "        current_best = value\n",
    "        best_trial_index = i\n",
    "    best_so_far.append(current_best)\n",
    "\n",
    "# Best trial info\n",
    "best_trial = trials[best_trial_index]\n",
    "best_params = best_trial.params\n",
    "best_value = best_trial.value\n",
    "\n",
    "# Round numeric parameters to 3 decimals\n",
    "param_text = \"\\n\".join([f\"{k}: {round(v,3) if isinstance(v,(int,float)) else v}\" \n",
    "                        for k, v in best_params.items()])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Scatter trial values\n",
    "ax.scatter(trial_numbers, objective_values, color='skyblue', label='Trial Values', edgecolor='k')\n",
    "\n",
    "# Best-so-far line\n",
    "ax.plot(trial_numbers, best_so_far, color='red', linewidth=2, label='Best-So-Far')\n",
    "\n",
    "# Highlight best trial\n",
    "ax.scatter(best_trial.number, best_value, color='green', s=150, marker='*', label='Best Trial', edgecolor='k')\n",
    "\n",
    "# Annotate best trial with a box\n",
    "ax.annotate(f'Best Trial #{best_trial.number}\\nValue: {best_value:.4f}\\n{param_text}',\n",
    "            xy=(best_trial.number, best_value),\n",
    "            xytext=(best_trial.number + 0.5, best_value + 0.5),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightyellow\", alpha=0.4),\n",
    "            fontsize=10)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Objective Value')\n",
    "ax.set_title('Optuna XGBoost Optimization Convergence')\n",
    "\n",
    "# Legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Compute residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Define extreme outliers: top 5% of absolute residuals\n",
    "threshold = np.percentile(np.abs(residuals), 95)\n",
    "outliers_mask = np.abs(residuals) >= threshold\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot normal listings\n",
    "plt.scatter(\n",
    "    y_test[~outliers_mask],\n",
    "    y_test_pred[~outliers_mask],\n",
    "    alpha=0.5,\n",
    "    color='skyblue',\n",
    "    label=\"Normal listings\",\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Plot extreme residuals\n",
    "plt.scatter(\n",
    "    y_test[outliers_mask],\n",
    "    y_test_pred[outliers_mask],\n",
    "    color=\"red\",\n",
    "    label=\"Extreme listings\",\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Diagonal line (perfect prediction)\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "plt.plot([0, max_val], [0, max_val], color=\"black\", linestyle=\"--\", label=\"Perfect prediction\")\n",
    "\n",
    "# Annotate a few largest outliers by absolute residual\n",
    "num_annotations = 5\n",
    "outliers_df = pd.DataFrame({\n",
    "    'y_true': y_test[outliers_mask],\n",
    "    'y_pred': y_test_pred[outliers_mask],\n",
    "    'residual': residuals[outliers_mask],\n",
    "    'size': X_test.loc[outliers_mask, 'size_num']  # replace 'size' with your feature name\n",
    "})\n",
    "outliers_df['abs_residual'] = np.abs(outliers_df['residual'])\n",
    "top_outliers = outliers_df.nlargest(num_annotations, 'abs_residual').copy()\n",
    "\n",
    "# Initialize annotation positions slightly offset\n",
    "top_outliers['x_offset'] = 10\n",
    "top_outliers['y_offset'] = 10\n",
    "\n",
    "# Simple repel for overlapping annotations\n",
    "min_distance = 15  # in points\n",
    "for i in range(len(top_outliers)):\n",
    "    for j in range(i):\n",
    "        dx = top_outliers.iloc[i]['x_offset'] - top_outliers.iloc[j]['x_offset']\n",
    "        dy = top_outliers.iloc[i]['y_offset'] - top_outliers.iloc[j]['y_offset']\n",
    "        distance = np.hypot(dx, dy)\n",
    "        if distance < min_distance:\n",
    "            top_outliers.at[top_outliers.index[i], 'y_offset'] += min_distance - distance\n",
    "\n",
    "# Add annotations with repelling offsets\n",
    "for idx, row in top_outliers.iterrows():\n",
    "    plt.annotate(\n",
    "        f\"Size: {row['size']}\\nPrice: €{row['y_true']:,.0f}\",\n",
    "        xy=(row['y_true'], row['y_pred']),\n",
    "        xytext=(row['x_offset'], row['y_offset']),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightyellow\", alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1)\n",
    "    )\n",
    "\n",
    "# Format axes in euros with thousands separator\n",
    "formatter = FuncFormatter(lambda x, _: f\"€{int(x):,}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Labels, title, legend\n",
    "plt.xlabel(\"Actual Price (€)\")\n",
    "plt.ylabel(\"Predicted Price (€)\")\n",
    "plt.title(\"Predicted vs Actual Prices with Extreme Listings Highlighted\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1460742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
