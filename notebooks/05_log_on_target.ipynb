{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc88793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "tracking_uri = \"../logs/mlruns\"\n",
    "os.makedirs(os.path.join(tracking_uri, \".trash\"), exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"house_price_prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Adjust the path to your project root folder\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(\"..\")\n",
    ")  # from notebooks/ up one level\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.data_loading.data_loading.data_loader import load_data_from_json\n",
    "from src.data_loading.preprocessing.preprocessing import preprocess_df\n",
    "from src.data_loading.preprocessing.imputation import impute_missing_values\n",
    "\n",
    "\n",
    "# go two levels up from notebook dir -> project root\n",
    "ROOT = (\n",
    "    Path(__file__).resolve().parents[2]\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parents[1]\n",
    ")\n",
    "CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"preprocessing_config.yaml\"\n",
    ")\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "df_raw = load_data_from_json(\"../data/parsed_json/*.json\")\n",
    "df_clean = preprocess_df(\n",
    "    df_raw,\n",
    "    drop_raw=CONFIG[\"preprocessing\"][\"drop_raw\"],\n",
    "    numeric_cols=CONFIG[\"preprocessing\"][\"numeric_cols\"],\n",
    ")\n",
    "df_clean = impute_missing_values(\n",
    "    df_clean, CONFIG[\"preprocessing\"][\"imputation\"]\n",
    ")\n",
    "# Drop price_num NaNs for the training of the model\n",
    "df_clean = df_clean[df_clean[\"price_num\"].notna()]\n",
    "df_clean.drop(columns=[\"living_area\"], inplace=True)\n",
    "\n",
    "\n",
    "# df_clean = df_clean[:100] \n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ad5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.data_prep_for_modelling.data_preparation import prepare_data\n",
    "\n",
    "FEATURES_CONFIG_PATH = (\n",
    "    ROOT / \"house_price_prediction_project\" / \"config\" / \"model_config.yaml\"\n",
    ")\n",
    "\n",
    "# Scaled features (applies scaling according to YAML)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, X_val, y_val, scaler, _ = prepare_data(\n",
    "    df,\n",
    "    config_path=FEATURES_CONFIG_PATH,\n",
    "    model_name=\"linear_regression\",  # uses the unified YAML key\n",
    "    use_extended_features=False,       # set True if you want extended features\n",
    "    cv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1263882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.evaluate import ModelEvaluator\n",
    "from src.model.mlflow_logger import MLFlowLogger\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "logger = MLFlowLogger()\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Evaluate\n",
    "trained_lr, y_train_pred, y_val_pred, y_test_pred, lr_results = evaluator.evaluate(\n",
    "    model=lr_model,\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_scaled,\n",
    "    y_test=y_test,\n",
    "    model_params={},   \n",
    "    fit_params={},     \n",
    "    use_xgb_train=False\n",
    ")\n",
    "\n",
    "# Log the model and results\n",
    "logger.log_model(trained_lr, \"LinearRegression\", lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "# --- Prepare data for final modeling ---\n",
    "X_train, X_test, y_train, y_test, X_val, y_val, scaler, feature_encoders = prepare_data(\n",
    "    df=df_clean,\n",
    "    config_path=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_name=\"xgboost_early_stopping\",  \n",
    "    use_extended_features=True,           \n",
    "    cv=False                              \n",
    ")\n",
    "\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape if X_val is not None else None)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa75e1",
   "metadata": {},
   "source": [
    "#### Without LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.copy()\n",
    "X_test_final = X_test.copy()\n",
    "X_val_final = X_val.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train_final.shape)\n",
    "print(\"validation shape:\", X_val_final.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with log-transform\n",
    "\n",
    "from functools import partial\n",
    "from src.model.objectives_optuna import unified_objective\n",
    "\n",
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "study_xgb = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "objective_xgb_partial = partial(\n",
    "    unified_objective,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng\",\n",
    "    df=df_clean,\n",
    "    features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    use_log=False,  \n",
    "    n_splits=5,\n",
    "    use_extended_features=True\n",
    ")\n",
    "study_xgb.optimize(objective_xgb_partial, n_trials=50)\n",
    "\n",
    "# # Random Forest with log-transform\n",
    "# study_rf = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# objective_rf_partial = partial(\n",
    "#     unified_objective,\n",
    "#     model_name=\"random_forest_optuna_feature_eng\",\n",
    "#     df=df_clean,\n",
    "#     features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "#     model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "#     use_log=True,  \n",
    "#     n_splits=5,\n",
    "#     use_extended_features=True\n",
    "\n",
    "# )\n",
    "\n",
    "# study_rf.optimize(objective_rf_partial, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator with log-transform if used\n",
    "# evaluator = ModelEvaluator(target_transform=np.log1p, inverse_transform=np.expm1)\n",
    "\n",
    "# # --- Random Forest ---\n",
    "# best_rf = RandomForestRegressor(**study_rf.best_params)\n",
    "# trained_rf, y_train_pred, y_val_pred, y_test_pred, results_rf = evaluator.evaluate(\n",
    "#     model=best_rf,\n",
    "#     X_train=X_train_final,\n",
    "#     y_train=y_train,\n",
    "#     X_test=X_test_final,\n",
    "#     y_test=y_test,\n",
    "#     X_val=X_val_final,\n",
    "#     y_val=y_val,\n",
    "#     use_xgb_train=False,\n",
    "# )\n",
    "# logger.log_model(trained_rf, \"RF_LogTransform_Optuna_feature_eng\", results_rf, use_xgb_train=False)\n",
    "\n",
    "# --- XGBoost ---\n",
    "best_xgb_params = study_xgb.best_params\n",
    "trained_xgb, y_train_pred, y_val_pred, y_test_pred, results_xgb = evaluator.evaluate(\n",
    "    model=None,  # not used in XGBoost.train\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_final,\n",
    "    y_test=y_test,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    use_xgb_train=True,\n",
    "    model_params=best_xgb_params,  # <--- crucial\n",
    "    fit_params={\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    ")\n",
    "logger.log_model(trained_xgb, \"XGB_Optuna_LogTransformed_feature_eng\", results_xgb, use_xgb_train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc0bcd",
   "metadata": {},
   "source": [
    "### with LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with log-transform\n",
    "\n",
    "from functools import partial\n",
    "from src.model.objectives_optuna import unified_objective\n",
    "\n",
    "FEATURES_AND_MODEL_CONFIG_PATH = (\n",
    "    ROOT\n",
    "    / \"house_price_prediction_project\"\n",
    "    / \"config\"\n",
    "    / \"model_config.yaml\"\n",
    ")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "study_xgb = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "objective_xgb_partial = partial(\n",
    "    unified_objective,\n",
    "    model_name=\"xgboost_early_stopping_optuna_feature_eng\",\n",
    "    df=df_clean,\n",
    "    features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "    use_log=True,  \n",
    "    n_splits=5,\n",
    "    use_extended_features=True\n",
    ")\n",
    "study_xgb.optimize(objective_xgb_partial, n_trials=50)\n",
    "\n",
    "# # Random Forest with log-transform\n",
    "# study_rf = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# objective_rf_partial = partial(\n",
    "#     unified_objective,\n",
    "#     model_name=\"random_forest_optuna_feature_eng\",\n",
    "#     df=df_clean,\n",
    "#     features_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "#     model_config=FEATURES_AND_MODEL_CONFIG_PATH,\n",
    "#     use_log=True,  \n",
    "#     n_splits=5,\n",
    "#     use_extended_features=True\n",
    "\n",
    "# )\n",
    "\n",
    "# study_rf.optimize(objective_rf_partial, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator with log-transform if used\n",
    "evaluator = ModelEvaluator(target_transform=np.log1p, inverse_transform=np.expm1)\n",
    "\n",
    "# # --- Random Forest ---\n",
    "# best_rf = RandomForestRegressor(**study_rf.best_params)\n",
    "# trained_rf, y_train_pred, y_val_pred, y_test_pred, results_rf = evaluator.evaluate(\n",
    "#     model=best_rf,\n",
    "#     X_train=X_train_final,\n",
    "#     y_train=y_train,\n",
    "#     X_test=X_test_final,\n",
    "#     y_test=y_test,\n",
    "#     X_val=X_val_final,\n",
    "#     y_val=y_val,\n",
    "#     use_xgb_train=False,\n",
    "# )\n",
    "# logger.log_model(trained_rf, \"RF_LogTransform_Optuna_feature_eng\", results_rf, use_xgb_train=False)\n",
    "\n",
    "# --- XGBoost ---\n",
    "best_xgb_params = study_xgb.best_params\n",
    "trained_xgb, y_train_pred, y_val_pred, y_test_pred, results_xgb = evaluator.evaluate(\n",
    "    model=None,  # not used in XGBoost.train\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_final,\n",
    "    y_test=y_test,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    use_xgb_train=True,\n",
    "    model_params=best_xgb_params,  # <--- crucial\n",
    "    fit_params={\"num_boost_round\": 1000, \"early_stopping_rounds\": 50},\n",
    ")\n",
    "logger.log_model(trained_xgb, \"XGB_Optuna_LogTransformed_feature_eng\", results_xgb, use_xgb_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa53ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from src.model.cv_helpers import prepare_base_data\n",
    "from src.features.feature_engineering import feature_engineering_cv as fe_cv\n",
    "\n",
    "prepare_fold_features = fe_cv.prepare_fold_features\n",
    "\n",
    "# Number of folds (should match your CV setup)\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Prepare base data (features + target)\n",
    "X_full, y_full = prepare_base_data(df_clean, FEATURES_AND_MODEL_CONFIG_PATH, \"xgboost_early_stopping_optuna_feature_eng\")\n",
    "\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "fold_importances = {imp_type: [] for imp_type in importance_types}\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "    X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "    y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "    \n",
    "    # Prepare fold-wise features (ensure extended feature engineering matches training)\n",
    "    X_train_fold, X_val_fold, _, _ = prepare_fold_features(X_train, X_val, use_extended_features=True)\n",
    "\n",
    "    cols_to_drop = [\"price_per_m2_neighborhood\", \"size_num\"]  # or just one of them\n",
    "    X_train_fold = X_train_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    X_val_fold = X_val_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Train XGBoost on this fold using best params\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=np.log1p(y_train))\n",
    "    dval = xgb.DMatrix(X_val_fold, label=np.log1p(y_val))\n",
    "    \n",
    "    model_fold = xgb.train(\n",
    "        params=best_xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Collect importance for each type\n",
    "    for imp_type in importance_types:\n",
    "        imp_dict = model_fold.get_score(importance_type=imp_type)\n",
    "        df_imp = pd.DataFrame.from_dict(imp_dict, orient=\"index\", columns=[imp_type])\n",
    "        df_imp.index.name = \"feature\"\n",
    "        fold_importances[imp_type].append(df_imp)\n",
    "\n",
    "# --- Aggregate across folds ---\n",
    "agg_importances = {}\n",
    "for imp_type, dfs in fold_importances.items():\n",
    "    # Combine all folds into a single dataframe\n",
    "    df_all = pd.concat(dfs, axis=1).fillna(0)\n",
    "    df_all[\"mean\"] = df_all.mean(axis=1)\n",
    "    df_all = df_all.sort_values(by=\"mean\", ascending=False)\n",
    "    agg_importances[imp_type] = df_all\n",
    "    print(f\"\\nTop 10 features by mean {imp_type} across folds:\")\n",
    "    print(df_all[\"mean\"].head(100))\n",
    "    \n",
    "    # Plot top 20\n",
    "    df_all[\"mean\"].head(200).plot.barh(figsize=(10,6), title=f\"Top 20 features by mean {imp_type} across folds\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "fold_shap_values = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
    "    X_train, X_val = X_full.iloc[train_idx].copy(), X_full.iloc[val_idx].copy()\n",
    "    y_train, y_val = y_full.iloc[train_idx].copy(), y_full.iloc[val_idx].copy()\n",
    "    \n",
    "    # Prepare fold-wise features\n",
    "    X_train_fold, X_val_fold, _, _ = prepare_fold_features(X_train, X_val, use_extended_features=True)\n",
    "    cols_to_drop = [\"price_per_m2_neighborhood\", \"size_num\"]  # or just one of them\n",
    "    X_train_fold = X_train_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    X_val_fold = X_val_fold.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Train XGBoost Booster\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=np.log1p(y_train))\n",
    "    dval = xgb.DMatrix(X_val_fold, label=np.log1p(y_val))\n",
    "    \n",
    "    model_fold = xgb.train(\n",
    "        params=best_xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # SHAP for Booster\n",
    "    explainer = shap.TreeExplainer(model_fold)\n",
    "    shap_values = explainer.shap_values(dval)  # DMatrix\n",
    "    \n",
    "    # Store mean absolute SHAP values per feature for this fold\n",
    "    fold_shap_values.append(pd.DataFrame({\n",
    "        \"feature\": X_val_fold.columns,\n",
    "        \"mean_abs_shap\": np.abs(shap_values).mean(axis=0)\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Combine all folds\n",
    "df_shap_all = pd.concat(fold_shap_values)\n",
    "\n",
    "# Group by feature and compute mean across folds\n",
    "agg_shap = df_shap_all.groupby(\"feature\")[\"mean_abs_shap\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by mean absolute SHAP value across folds:\")\n",
    "print(agg_shap.head(10))\n",
    "\n",
    "# Prepare data\n",
    "top_features = agg_shap.head(10).sort_values()\n",
    "features = top_features.index\n",
    "shap_values = top_features.values\n",
    "models = np.arange(len(features))\n",
    "bar_width = 0.6\n",
    "color = 'skyblue'\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Horizontal bar plot\n",
    "ax.barh(models, shap_values, height=bar_width, color=color)\n",
    "\n",
    "# Y-axis labels\n",
    "ax.set_yticks(models)\n",
    "ax.set_yticklabels(features)\n",
    "ax.invert_yaxis()  # highest SHAP on top\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Mean Absolute SHAP Value\")\n",
    "ax.set_title(\"Top 10 Features by SHAP Value\")\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(shap_values):\n",
    "    ax.text(val + 0.01 * max(shap_values), i, f\"{val:.3f}\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all folds\n",
    "df_shap_all = pd.concat(fold_shap_values)\n",
    "\n",
    "# Group by feature and compute mean across folds\n",
    "agg_shap = df_shap_all.groupby(\"feature\")[\"mean_abs_shap\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features by mean absolute SHAP value across folds:\")\n",
    "print(agg_shap.head(10))\n",
    "\n",
    "# Plot\n",
    "agg_shap.head(10).sort_values().plot.barh(figsize=(10,6), title=\"Top 10 features by SHAP value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "def shap_to_euros(model, X, shap_values_array, target_transform_inverse=np.expm1):\n",
    "    \"\"\"\n",
    "    Convert SHAP values from log1p scale to original price scale.\n",
    "\n",
    "    Args:\n",
    "        model: trained XGBoost model (xgb.Booster)\n",
    "        X: pd.DataFrame, features used for prediction\n",
    "        shap_values_array: numpy array of SHAP values (n_samples, n_features)\n",
    "        target_transform_inverse: function to revert log1p, default np.expm1\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: mean absolute SHAP contributions in euros per feature\n",
    "    \"\"\"\n",
    "    # Predict log1p(price) for each sample\n",
    "    dmatrix = xgb.DMatrix(X)\n",
    "    pred_log = model.predict(dmatrix)  # log1p(price)\n",
    "    \n",
    "    # Approximate contribution in original price units\n",
    "    # delta_euros = exp(pred + shap) - exp(pred)\n",
    "    price_contrib = np.expm1(pred_log[:, None] + shap_values_array) - np.expm1(pred_log[:, None])\n",
    "    \n",
    "    # Convert to DataFrame for convenience\n",
    "    df_shap_euros = pd.DataFrame(price_contrib, columns=X.columns)\n",
    "    \n",
    "    # Mean absolute contribution per feature\n",
    "    mean_abs_euros = df_shap_euros.abs().mean().sort_values(ascending=False)\n",
    "    \n",
    "    return mean_abs_euros\n",
    "\n",
    "# Example usage:\n",
    "mean_abs_shap_euros = shap_to_euros(model_fold, X_val_fold, shap_values.values if hasattr(shap_values, \"values\") else shap_values)\n",
    "print(\"Top 20 features by SHAP contribution in euros:\")\n",
    "print(mean_abs_shap_euros.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b321d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Top 10 features\n",
    "top_features = mean_abs_shap_euros.head(10).sort_values()\n",
    "features = top_features.index\n",
    "shap_values = top_features.values\n",
    "models = np.arange(len(features))\n",
    "bar_width = 0.6\n",
    "color = 'skyblue'\n",
    "\n",
    "# Friendly feature names\n",
    "feature_rename = {\n",
    "    \"log_size_num\": \"Property Size (log m²)\",\n",
    "    \"log_price_per_m2_neighborhood\": \"Price per m² by Neighborhood (log €)\",\n",
    "    \"nr_rooms\": \"Number of Rooms\",\n",
    "    \"bathrooms\": \"Number of Bathrooms\",\n",
    "    \"energy_label_encoded\": \"Energy Label\",\n",
    "    \"postal_district_110\": \"Postal District 110\",\n",
    "    \"garden_None\": \"No Garden\",\n",
    "    \"room_utilization_ratio\": \"Room Utilization Ratio\",\n",
    "    \"neighborhood_facility_density\": \"Neighborhood Amenities Density\",\n",
    "    \"num_facilities\": \"Number of Facitilies\"\n",
    "}\n",
    "\n",
    "# Notes dictionary\n",
    "notes_dict = {\n",
    "    \"log_size_num\": \"Larger homes dominate price predictions.\",\n",
    "    \"log_price_per_m2_neighborhood\": \"Captures local market variation.\",\n",
    "    \"nr_rooms\": \"More rooms generally increase price.\",\n",
    "    \"bathrooms\": \"Bathrooms add significant value.\",\n",
    "    \"energy_label_encoded\": \"Energy efficiency materially affects price.\",\n",
    "    \"postal_district_110\": \"Micro-location matters.\",\n",
    "    \"garden_None\": \"Lack of a garden slightly lowers value.\",\n",
    "    \"room_utilization_ratio\": \"Efficient space usage boosts price per m².\",\n",
    "    \"neighborhood_facility_density\": \"Access to amenities increases home value.\",\n",
    "    \"num_facilities\": \"Larger number of facilities increases the price.\"\n",
    "\n",
    "}\n",
    "\n",
    "# Map friendly names\n",
    "friendly_names = [feature_rename.get(f, f) for f in features]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(14, 6))  # wider for notes\n",
    "\n",
    "# Horizontal bar plot\n",
    "bars = ax.barh(models, shap_values, height=bar_width, color=color)\n",
    "\n",
    "# Y-axis labels (friendly names)\n",
    "ax.set_yticks(models)\n",
    "ax.set_yticklabels(friendly_names)\n",
    "ax.invert_yaxis()  # highest SHAP on top\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Mean Absolute SHAP Contribution (€)\")\n",
    "ax.set_title(\"Top 10 Features by SHAP Contribution in Euros\")\n",
    "\n",
    "# Determine far-right position for notes (outside all bars)\n",
    "note_x = max(shap_values) * 1.15\n",
    "\n",
    "# Add value labels on bars and notes in boxes on the right\n",
    "for i, bar in enumerate(bars):\n",
    "    val = bar.get_width()\n",
    "    # Bar value\n",
    "    ax.text(val + max(shap_values)*0.01, i, f\"€{val:,.0f}\", va='center', fontsize=10)\n",
    "    \n",
    "    # Note, if exists\n",
    "    note = notes_dict.get(features[i], \"\")\n",
    "    if note:\n",
    "        ax.text(note_x, i, note, va='center', ha='left', fontsize=9,\n",
    "                color='black', fontstyle='italic',\n",
    "                bbox=dict(facecolor='lightgray', alpha=0.3, boxstyle='round,pad=0.3'))\n",
    "\n",
    "# Extend x-limits to fit notes\n",
    "ax.set_xlim(0, note_x * 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Sort trials by number\n",
    "trials = sorted(study_xgb.trials, key=lambda t: t.number)\n",
    "trial_numbers = [t.number for t in trials]\n",
    "objective_values = [t.value for t in trials]\n",
    "\n",
    "# Compute best-so-far values\n",
    "best_so_far = []\n",
    "current_best = float('inf')  # minimizing objective\n",
    "best_trial_index = 0\n",
    "for i, value in enumerate(objective_values):\n",
    "    if value < current_best:\n",
    "        current_best = value\n",
    "        best_trial_index = i\n",
    "    best_so_far.append(current_best)\n",
    "\n",
    "# Best trial info\n",
    "best_trial = trials[best_trial_index]\n",
    "best_params = best_trial.params\n",
    "best_value = best_trial.value\n",
    "\n",
    "# Round numeric parameters to 3 decimals\n",
    "param_text = \"\\n\".join([f\"{k}: {round(v,3) if isinstance(v,(int,float)) else v}\" \n",
    "                        for k, v in best_params.items()])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Scatter trial values\n",
    "ax.scatter(trial_numbers, objective_values, color='skyblue', label='Trial Values', edgecolor='k')\n",
    "\n",
    "# Best-so-far line\n",
    "ax.plot(trial_numbers, best_so_far, color='red', linewidth=2, label='Best-So-Far')\n",
    "\n",
    "# Highlight best trial\n",
    "ax.scatter(best_trial.number, best_value, color='green', s=150, marker='*', label='Best Trial', edgecolor='k')\n",
    "\n",
    "# Annotate best trial with a box\n",
    "ax.annotate(f'Best Trial #{best_trial.number}\\nValue: {best_value:.4f}\\n{param_text}',\n",
    "            xy=(best_trial.number, best_value),\n",
    "            xytext=(best_trial.number + 0.5, best_value + 0.5),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightyellow\", alpha=0.4),\n",
    "            fontsize=10)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Objective Value')\n",
    "ax.set_title('Optuna XGBoost Optimization Convergence')\n",
    "\n",
    "# Legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Compute residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Define extreme outliers: top 5% of absolute residuals\n",
    "threshold = np.percentile(np.abs(residuals), 95)\n",
    "outliers_mask = np.abs(residuals) >= threshold\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot normal listings\n",
    "plt.scatter(\n",
    "    y_test[~outliers_mask],\n",
    "    y_test_pred[~outliers_mask],\n",
    "    alpha=0.5,\n",
    "    color='skyblue',\n",
    "    label=\"Normal listings\",\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Plot extreme residuals\n",
    "plt.scatter(\n",
    "    y_test[outliers_mask],\n",
    "    y_test_pred[outliers_mask],\n",
    "    color=\"red\",\n",
    "    label=\"Extreme listings\",\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Diagonal line (perfect prediction)\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "plt.plot([0, max_val], [0, max_val], color=\"black\", linestyle=\"--\", label=\"Perfect prediction\")\n",
    "\n",
    "# Annotate a few largest outliers by absolute residual\n",
    "num_annotations = 5\n",
    "outliers_df = pd.DataFrame({\n",
    "    'y_true': y_test[outliers_mask],\n",
    "    'y_pred': y_test_pred[outliers_mask],\n",
    "    'residual': residuals[outliers_mask],\n",
    "    'size': X_test.loc[outliers_mask, 'size_num']  # replace 'size' with your feature name\n",
    "})\n",
    "outliers_df['abs_residual'] = np.abs(outliers_df['residual'])\n",
    "top_outliers = outliers_df.nlargest(num_annotations, 'abs_residual').copy()\n",
    "\n",
    "# Initialize annotation positions slightly offset\n",
    "top_outliers['x_offset'] = 10\n",
    "top_outliers['y_offset'] = 10\n",
    "\n",
    "# Simple repel for overlapping annotations\n",
    "min_distance = 15  # in points\n",
    "for i in range(len(top_outliers)):\n",
    "    for j in range(i):\n",
    "        dx = top_outliers.iloc[i]['x_offset'] - top_outliers.iloc[j]['x_offset']\n",
    "        dy = top_outliers.iloc[i]['y_offset'] - top_outliers.iloc[j]['y_offset']\n",
    "        distance = np.hypot(dx, dy)\n",
    "        if distance < min_distance:\n",
    "            top_outliers.at[top_outliers.index[i], 'y_offset'] += min_distance - distance\n",
    "\n",
    "# Add annotations with repelling offsets\n",
    "for idx, row in top_outliers.iterrows():\n",
    "    plt.annotate(\n",
    "        f\"Size: {row['size']}\\nPrice: €{row['y_true']:,.0f}\",\n",
    "        xy=(row['y_true'], row['y_pred']),\n",
    "        xytext=(row['x_offset'], row['y_offset']),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightyellow\", alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1)\n",
    "    )\n",
    "\n",
    "# Format axes in euros with thousands separator\n",
    "formatter = FuncFormatter(lambda x, _: f\"€{int(x):,}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Labels, title, legend\n",
    "plt.xlabel(\"Actual Price (€)\")\n",
    "plt.ylabel(\"Predicted Price (€)\")\n",
    "plt.title(\"Predicted vs Actual Prices with Extreme Listings Highlighted\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1460742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
